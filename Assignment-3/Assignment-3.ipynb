{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: ICP + Non-linear least squares optimization\n",
    "\n",
    "TEAM-NAME: \n",
    "\n",
    "YOUR-ID: \n",
    "\n",
    "YOUR-NAME:\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* You are not allowed to use any external libraries (other than ones being imported below).\n",
    "* The deadline for this assignment is **15-09-21** at 11:55pm.\n",
    "* Plagiarism is **strictly prohibited**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linear Least Squares Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Gradient Descent\n",
    "Implement the gradient descent algorithm using numpy and what you have learned from class to solve for the parameters of a gaussian distribution.\n",
    "To understand the task in more detail and look at a worked through example, checkout the subsequent section. You have to implement the same using just numpy functions. You can refer to [Shubodh's notes](https://www.notion.so/saishubodh/From-linear-algebra-to-non-linear-weighted-least-squares-optimization-13cf17d318be4d45bb8577c4d3ea4a02) on the same to get a better grasp of the concept before implementing it.\n",
    "* Experiment with the number of iterations.\n",
    "* Experiment with the learning rate.\n",
    "* Experiment with the tolerance.\n",
    "\n",
    "Display your results using matplotlib by plotting graphs for \n",
    "* The cost function value vs the number of iterations\n",
    "* The Ground Truth data values and the predicted data values.\n",
    "\n",
    "Your plots are expected to contain information similar to the plot below:\n",
    "\n",
    "<!-- <figure> -->\n",
    "<img src='./helpers/sample_plt.png' alt=drawing width=500 height=600>\n",
    "\n",
    "<!-- <figcaption align='center'><b>A sample plot, you can use your own plotting template</b></figcaption>\n",
    "</figure> -->\n",
    "<!-- head over to [this page](https://saishubodh.notion.site/Non-Linear-Least-Squares-Solved-example-Computing-Jacobian-for-a-Gaussian-Gradient-Descent-7fd11ebfee034f8ca89cc78c8f1d24d9) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked out Example using Gradient Descent\n",
    "\n",
    "A Gaussian distribution parametrized by $a,m,s$ is given by:\n",
    "\n",
    "$$ y(x;a,m,s)=a \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right) \\tag{1}$$\n",
    "\n",
    "### Jacobian of Gaussian\n",
    "\n",
    "$$\\mathbf{J}_y=\\left[\\frac{\\partial y}{\\partial a} \\quad \\frac{\\partial y}{\\partial m} \\quad \\frac{\\partial y}{\\partial s}\\right] \\\\\n",
    "= \\left[ \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right); \\frac{a (x-m)}{s^2} \\exp\\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right);  \\frac{a (x-m)^2}{s^3}\\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right)\\right]$$\n",
    "\n",
    "## Problem at hand\n",
    "\n",
    "> Given a set of observations $y_{obs}$ and $x_{obs}$ we want to find the optimum parameters $a,m,s$ which best fit our observations given an initial estimate.\n",
    "\n",
    "Our observations would generally be erroneous and given to us, but for the sake of knowing how good our model is performing, let us generate the observations ourselves by assuming the actual \"actual\" parameter values as $a_{gt}=10; m_{gt} =0; s_{gt} =20$ ($gt$ stands for ground truth). We will try to estimate these values based on our observations and let us see how close we get to \"actual\" parameters. Note that in reality we obviously don't have these parameters as that is exactly what we want to estimate in the first place. So let us consider the following setup, we have:\n",
    "\n",
    "- Number of observations, $num\\_obs = 50$\n",
    "- Our 50 set of observations would be\n",
    "    - $x_{obs} = np.linspace(-25,25, num\\_obs)$\n",
    "    - $y_{obs} = y(x_{obs};a_{gt},m_{gt},s_{gt})$  from $(1)$\n",
    "\n",
    "Reference:\n",
    "\n",
    "â†’[linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)\n",
    "\n",
    "- Say we are given initial estimate as:\n",
    "\n",
    "    $$a_0=10; \\quad m_0=13; \\quad s_0=19.12$$\n",
    "\n",
    "### Residual and error to be minimized\n",
    "\n",
    "Okay, now we have set of observations and an initial estimate of parameters. We would now want to minimize an error that would give us optimum parameters.\n",
    "\n",
    "The $residual$ would be given by\n",
    "\n",
    "$$ r(a,m,s) = \\left[ a \\exp \\left(\\frac{-(x_{obs}-m)^{2}}{2 s^{2}}\\right) - y_{obs}\\ \\right]$$\n",
    "\n",
    "where we'd want to minimize $\\|r\\|^2$. Note that $r$ is a non-linear function in $(a,m,s)$.\n",
    "\n",
    "Also, note that since $y$ (and $x$) are observations in the above equation, after simplification, we get $\\mathbf{J}_r = \\mathbf{J}_y$ [above](https://www.notion.so/c9e6f71b67a44bb8b366df2fccfc12d0) (since $y_{obs}$ is a constant).\n",
    "\n",
    "Let us apply Gradient Descent method for minimization here. From [Table I](https://www.notion.so/From-linear-algebra-to-non-linear-weighted-least-squares-optimization-13cf17d318be4d45bb8577c4d3ea4a02),  \n",
    "\n",
    "$$\\Delta \\mathbf{k} = - \\alpha \\mathbf{J_F} = -\\alpha \\mathbf{J}_r^{\\top} {r}(\\mathbf{k})$$\n",
    "\n",
    "Note that $\\mathbf{J_F}$ is the Jacobian of \"non-linear least squares\" function $\\mathbf{F}$ while $\\mathbf{J}_r$ is the Jacobian of the residual. \n",
    "\n",
    "where $\\mathbf{k}$ is $[a,m,s]^T$. \n",
    "\n",
    "- Some hyperparameters:\n",
    "    - Learning rate, $lr = 0.01$\n",
    "    - Maximum number of iterations, $num\\_iter=200$\n",
    "    - Tolerance, $tol = 1e-15$\n",
    "\n",
    "## Solution for 1 iteration\n",
    "\n",
    "To see how each step looks like, let us solve for 1 iteration and for simpler calculations, assume we have 3 observations, \n",
    "\n",
    "$$x_{obs}= \\left[ -25, 0, 25 \\right]^T, y_{obs} = \\left[  4.5783, 10, 4.5783 \\right]^T. $$\n",
    "\n",
    "With our initial estimate as $\\mathbf{k_0} = [a_0=10, \\quad m_0=13, \\quad s_0=19.12]^T$, the residual would be \n",
    "\n",
    "$$ r(a_0,m_0,s_0) = \\left[ a_0 \\exp \\left(\\frac{-(x_{obs}-m_0)^{2}}{2 s_0^{2}}\\right) - y_{obs}\\ \\right]$$\n",
    "\n",
    "Therefore, $r=[-3.19068466, -2.0637411 , 3.63398058]^T$.\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Gradient, $\\mathbf{J_F}$=\n",
    "\n",
    "$$\\mathbf{J_r}^{\\top} \\mathbf{r}(\\mathbf{k})$$\n",
    "\n",
    "We have calculated residual already [above](https://www.notion.so/c9e6f71b67a44bb8b366df2fccfc12d0), let us calculate the Jacobian $\\mathbf{J_r}$.\n",
    "\n",
    "$$\\mathbf{J}_r\n",
    "= \\left[ \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right); \\frac{a (x-m)}{s^2} \\exp\\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right);  \\frac{a (x-m)^2}{s^3}\\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right)\\right]$$\n",
    "\n",
    "$$\\implies \\mathbf{J_r} = \\left[ \\begin{array}{rrr}0.1387649 & 0.79362589, & 0.82123142 \\\\-0.14424057 & -0.28221715  & 0.26956967 \\\\0.28667059 & 0.19188405, & 0.16918599\\end{array}\\right]$$\n",
    "\n",
    "So ,\n",
    "\n",
    "$$\\mathbf{J_F} = \\mathbf{J_r}^{\\top} \\mathbf{r}(\\mathbf{k})$$\n",
    "\n",
    "$$\\mathbf{r}(\\mathbf{k}) =  \\left[ \\begin{array}{r}-3.19068466 \\\\ -2.0637411 \\\\ 3.63398058 \\end{array} \\right]$$\n",
    "\n",
    "$$ \\begin{aligned} \\implies \\mathbf{J_F} = \\left[ \\begin{array}{r} 0.89667553 \\\\ -1.25248392 \\\\-2.56179392\\end{array} \\right] \\end{aligned}$$\n",
    "\n",
    "### Update step\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{k} = - \\alpha \\mathbf{J_F} \\\\\n",
    "\\mathbf{k}^{t+1} = \\mathbf{k}^t + \\Delta \\mathbf{k}\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ our learning rate is 0.01.\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{k} = - \\alpha\\times\\left[ \\begin{array}{r} \n",
    "0.89667553 \\\\ -1.25248392 \\\\-2.56179392\n",
    "\\end{array} \\right] = \\left[ \\begin{array}{r}\n",
    "-0.00896676 \\\\ 0.01252484 \\\\0.02561794\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{k}^{1} = \\mathbf{k}^{0} + \\Delta \\mathbf{k} \\\\ \\left[\\begin{array}{r} 10 \\\\ 13 \\\\ 19.12 \\end{array}\\right] + \\left[\\begin{array}{c} 9.99103324 \\\\ 13.01252484 \\\\ 19.14561794 \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "With just one iteration with very few observations, we can see that we have gotten *slightly* more closer to our GT parameter  $a_{gt}=10; m_{gt} =0; s_{gt} =20$. Our initial estimate was $[a_0=10, \\quad m_0=13, \\quad s_0=19.12]$. However, the above might not be noticeable enough: Hence you need to code it for more iterations and convince yourself as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.func import make_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Another Non-Linear function\n",
    "Now that you've got the hang of computing the jacobian matrix for a non-linear function via the aid of an example, try to compute the jacobian of a secondary gaussian function by carrying out steps similar to what has been shown above. The function is plotted below:\n",
    "<img src='./helpers/non_linear.png' alt=drawing width=500 height=600>\n",
    "Using the computed jacobian, optimise for the four parameters using gradient descent, where the parameters to be estimated are: \n",
    "\n",
    "$p_1$ = 2,  $p_2$ = 8,  $p_3$ = 4,  $p_4$ = 8. \n",
    "\n",
    "Do this for $x_{obs} = np.linspace(-20,30, num\\_obs)$,\n",
    "where $num\\_obs$ is 50.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.func import make_non_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Different Optimizers\n",
    "\n",
    "Replace gradient descent with Gauss-Newton and Levenberg Marquardt algorithms and repeat question 1.1. \n",
    "\n",
    "To quickly recap, Gauss-Newton and Levenberg Marquardt are alternate update rules to the standard gradient descent. Gauss Newton updates work as:\n",
    "\n",
    "$$\\delta x = -(J^TJ)^{-1}J^Tf(x)$$\n",
    "\n",
    "Levenberg Marquardt lies somewhere between Gauss Newton and Gradient Descent algorithms by blending the two formulations. As a result, when at a steep cliff, LM takes small steps to avoid overshooting, and when at a gentle slope, LM takes bigger steps:\n",
    "\n",
    "\n",
    "$$\\delta x = -(J^TJ + \\lambda I)^{-1}J^Tf(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "   * 1. How does the choice of initial estimate and learning rate affect convergence? Observations and analysis from repeated runs with modified hyperparameters will suffice.\n",
    "   * 2. Do you notice any difference between the three optimizers? Why do you think that is? (If you are unable to see a clear trend, what would you expect in general based on what you know about them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iterative Closest Point\n",
    "\n",
    "In this subsection, we will code the Iterative Closest Point algorithm to find the alignment between two point clouds without known correspondences. The point cloud that you will be using is the same as the one that you used in Assignment 1.\n",
    "\n",
    "## 2.1: Procrustes alignment\n",
    "\n",
    "1. Write a function that takes two point clouds as input wherein the corresponding points between the two point clouds are located at the same index and returns the transformation matrix between them.\n",
    "2. Use the bunny point cloud and perform the procrustes alignment between the two bunnies. Compute the absolute alignment error after aligning the two bunnies.\n",
    "3. Make sure your code is modular as we will use this function in the next sub-part.\n",
    "4. Prove mathematically why the Procrustes alignment gives the best aligning transform between point clouds with known correspondences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: ICP alignment\n",
    "\n",
    "1. Write a function that takes two point clouds as input without known correspondences and perform the iterative closest point algorithm.\n",
    "2. Perform the ICP alignment between the two bunnies and plot their individual coordinate frames as done in class.\n",
    "3. Does ICP always give the correct alignment? Why or Why not?\n",
    "4. What are other variants of ICP and why are they helpful (you can look at point to plane ICP)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25835b948c24ebd10969a4ad606ca6028421d312f64f7f25ef4c39380c712bba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('mr_assignment3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
