{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: ICP + Non-linear least squares optimization\n",
    "\n",
    "TEAM-NAME: \n",
    "\n",
    "YOUR-ID: \n",
    "\n",
    "YOUR-NAME:\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* You are not allowed to use any external libraries (other than ones being imported below).\n",
    "* The deadline for this assignment is **15-09-21** at 11:55pm.\n",
    "* Plagiarism is **strictly prohibited**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linear Least Squares Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Gradient Descent\n",
    "Implement the gradient descent algorithm using numpy and what you have learned from class to solve for the parameters of a gaussian distribution.\n",
    "To understand the task in more detail and look at a worked through example, checkout the subsequent section. You have to implement the same using just numpy functions. You can refer to [Shubodh's notes](https://www.notion.so/saishubodh/From-linear-algebra-to-non-linear-weighted-least-squares-optimization-13cf17d318be4d45bb8577c4d3ea4a02) on the same to get a better grasp of the concept before implementing it.\n",
    "* Experiment with the number of iterations.\n",
    "* Experiment with the learning rate.\n",
    "* Experiment with the tolerance.\n",
    "\n",
    "Display your results using matplotlib by plotting graphs for \n",
    "* The cost function value vs the number of iterations\n",
    "* The Ground Truth data values and the predicted data values.\n",
    "\n",
    "Your plots are expected to contain information similar to the plot below:\n",
    "\n",
    "<!-- <figure> -->\n",
    "<img src='./helpers/sample_plt.png' alt=drawing width=500 height=600>\n",
    "\n",
    "<!-- <figcaption align='center'><b>A sample plot, you can use your own plotting template</b></figcaption>\n",
    "</figure> -->\n",
    "<!-- head over to [this page](https://saishubodh.notion.site/Non-Linear-Least-Squares-Solved-example-Computing-Jacobian-for-a-Gaussian-Gradient-Descent-7fd11ebfee034f8ca89cc78c8f1d24d9) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked out Example using Gradient Descent\n",
    "\n",
    "A Gaussian distribution parametrized by $a,m,s$ is given by:\n",
    "\n",
    "$$ y(x;a,m,s)=a \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right) \\tag{1}$$\n",
    "\n",
    "### Jacobian of Gaussian\n",
    "\n",
    "$$\\mathbf{J}_y=\\left[\\frac{\\partial y}{\\partial a} \\quad \\frac{\\partial y}{\\partial m} \\quad \\frac{\\partial y}{\\partial s}\\right] \\\\\n",
    "= \\left[ \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right); \\frac{a (x-m)}{s^2} \\exp\\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right);  \\frac{a (x-m)^2}{s^3}\\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right)\\right]$$\n",
    "\n",
    "## Problem at hand\n",
    "\n",
    "> Given a set of observations $y_{obs}$ and $x_{obs}$ we want to find the optimum parameters $a,m,s$ which best fit our observations given an initial estimate.\n",
    "\n",
    "Our observations would generally be erroneous and given to us, but for the sake of knowing how good our model is performing, let us generate the observations ourselves by assuming the actual \"actual\" parameter values as $a_{gt}=10; m_{gt} =0; s_{gt} =20$ ($gt$ stands for ground truth). We will try to estimate these values based on our observations and let us see how close we get to \"actual\" parameters. Note that in reality we obviously don't have these parameters as that is exactly what we want to estimate in the first place. So let us consider the following setup, we have:\n",
    "\n",
    "- Number of observations, $num\\_obs = 50$\n",
    "- Our 50 set of observations would be\n",
    "    - $x_{obs} = np.linspace(-25,25, num\\_obs)$\n",
    "    - $y_{obs} = y(x_{obs};a_{gt},m_{gt},s_{gt})$  from $(1)$\n",
    "\n",
    "Reference:\n",
    "\n",
    "â†’[linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)\n",
    "\n",
    "- Say we are given initial estimate as:\n",
    "\n",
    "    $$a_0=10; \\quad m_0=13; \\quad s_0=19.12$$\n",
    "\n",
    "### Residual and error to be minimized\n",
    "\n",
    "Okay, now we have set of observations and an initial estimate of parameters. We would now want to minimize an error that would give us optimum parameters.\n",
    "\n",
    "The $residual$ would be given by\n",
    "\n",
    "$$ r(a,m,s) = \\left[ a \\exp \\left(\\frac{-(x_{obs}-m)^{2}}{2 s^{2}}\\right) - y_{obs}\\ \\right]$$\n",
    "\n",
    "where we'd want to minimize $\\|r\\|^2$. Note that $r$ is a non-linear function in $(a,m,s)$.\n",
    "\n",
    "Also, note that since $y$ (and $x$) are observations in the above equation, after simplification, we get $\\mathbf{J}_r = \\mathbf{J}_y$ [above](https://www.notion.so/c9e6f71b67a44bb8b366df2fccfc12d0) (since $y_{obs}$ is a constant).\n",
    "\n",
    "Let us apply Gradient Descent method for minimization here. From [Table I](https://www.notion.so/From-linear-algebra-to-non-linear-weighted-least-squares-optimization-13cf17d318be4d45bb8577c4d3ea4a02),  \n",
    "\n",
    "$$\\Delta \\mathbf{k} = - \\alpha \\mathbf{J_F} = -\\alpha \\mathbf{J}_r^{\\top} {r}(\\mathbf{k})$$\n",
    "\n",
    "Note that $\\mathbf{J_F}$ is the Jacobian of \"non-linear least squares\" function $\\mathbf{F}$ while $\\mathbf{J}_r$ is the Jacobian of the residual. \n",
    "\n",
    "where $\\mathbf{k}$ is $[a,m,s]^T$. \n",
    "\n",
    "- Some hyperparameters:\n",
    "    - Learning rate, $lr = 0.01$\n",
    "    - Maximum number of iterations, $num\\_iter=200$\n",
    "    - Tolerance, $tol = 1e-15$\n",
    "\n",
    "## Solution for 1 iteration\n",
    "\n",
    "To see how each step looks like, let us solve for 1 iteration and for simpler calculations, assume we have 3 observations, \n",
    "\n",
    "$$x_{obs}= \\left[ -25, 0, 25 \\right]^T, y_{obs} = \\left[  4.5783, 10, 4.5783 \\right]^T. $$\n",
    "\n",
    "With our initial estimate as $\\mathbf{k_0} = [a_0=10, \\quad m_0=13, \\quad s_0=19.12]^T$, the residual would be \n",
    "\n",
    "$$ r(a_0,m_0,s_0) = \\left[ a_0 \\exp \\left(\\frac{-(x_{obs}-m_0)^{2}}{2 s_0^{2}}\\right) - y_{obs}\\ \\right]$$\n",
    "\n",
    "Therefore, $r=[-3.19068466, -2.0637411 , 3.63398058]^T$.\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Gradient, $\\mathbf{J_F}$=\n",
    "\n",
    "$$\\mathbf{J_r}^{\\top} \\mathbf{r}(\\mathbf{k})$$\n",
    "\n",
    "We have calculated residual already [above](https://www.notion.so/c9e6f71b67a44bb8b366df2fccfc12d0), let us calculate the Jacobian $\\mathbf{J_r}$.\n",
    "\n",
    "$$\\mathbf{J}_r\n",
    "= \\left[ \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right); \\frac{a (x-m)}{s^2} \\exp\\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right);  \\frac{a (x-m)^2}{s^3}\\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right)\\right]$$\n",
    "\n",
    "$$\\implies \\mathbf{J_r} = \\left[ \\begin{array}{rrr}0.1387649 & 0.79362589, & 0.82123142 \\\\-0.14424057 & -0.28221715  & 0.26956967 \\\\0.28667059 & 0.19188405, & 0.16918599\\end{array}\\right]$$\n",
    "\n",
    "So ,\n",
    "\n",
    "$$\\mathbf{J_F} = \\mathbf{J_r}^{\\top} \\mathbf{r}(\\mathbf{k})$$\n",
    "\n",
    "$$\\mathbf{r}(\\mathbf{k}) =  \\left[ \\begin{array}{r}-3.19068466 \\\\ -2.0637411 \\\\ 3.63398058 \\end{array} \\right]$$\n",
    "\n",
    "$$ \\begin{aligned} \\implies \\mathbf{J_F} = \\left[ \\begin{array}{r} 0.89667553 \\\\ -1.25248392 \\\\-2.56179392\\end{array} \\right] \\end{aligned}$$\n",
    "\n",
    "### Update step\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{k} = - \\alpha \\mathbf{J_F} \\\\\n",
    "\\mathbf{k}^{t+1} = \\mathbf{k}^t + \\Delta \\mathbf{k}\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ our learning rate is 0.01.\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{k} = - \\alpha\\times\\left[ \\begin{array}{r} \n",
    "0.89667553 \\\\ -1.25248392 \\\\-2.56179392\n",
    "\\end{array} \\right] = \\left[ \\begin{array}{r}\n",
    "-0.00896676 \\\\ 0.01252484 \\\\0.02561794\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{k}^{1} = \\mathbf{k}^{0} + \\Delta \\mathbf{k} \\\\ \\left[\\begin{array}{r} 10 \\\\ 13 \\\\ 19.12 \\end{array}\\right] + \\left[\\begin{array}{c} 9.99103324 \\\\ 13.01252484 \\\\ 19.14561794 \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "With just one iteration with very few observations, we can see that we have gotten *slightly* more closer to our GT parameter  $a_{gt}=10; m_{gt} =0; s_{gt} =20$. Our initial estimate was $[a_0=10, \\quad m_0=13, \\quad s_0=19.12]$. However, the above might not be noticeable enough: Hence you need to code it for more iterations and convince yourself as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Parameters\n",
    "\n",
    "actual_amp=10\n",
    "actual_mean=0\n",
    "actual_sd=20\n",
    "\n",
    "no_of_obs=50\n",
    "\n",
    "gd_learning_rate=10\n",
    "no_of_itr=1000\n",
    "tolerance=1e-10\n",
    "\n",
    "nonl_learning_rate=1e-6\n",
    "no_of_itr2=15000\n",
    "tolerance2=1e-15\n",
    "\n",
    "initial_amp,initia_mean,initial_sd=10,13,19.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.func import make_gaussian\n",
    "\n",
    "#generating data\n",
    "x_obs = np.linspace(-25, 25, no_of_obs)\n",
    "y_obs = make_gaussian(x_obs, actual_amp, actual_mean, actual_sd)\n",
    "\n",
    "def show_init_state():\n",
    "    y_est = make_gaussian(x_obs, initial_amp, initia_mean, initial_sd)\n",
    "    plt.plot(x_obs,y_obs)\n",
    "    plt.plot(x_obs,y_est)\n",
    "    plt.xlabel(\"X\"),plt.ylabel(\"Y\")\n",
    "    plt.legend([\"Ground Truth\",\"Initial Guess\"])\n",
    "    plt.show()\n",
    "\n",
    "show_init_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining partial derivatives\n",
    "def y_wrt_a(x,a,m,s):\n",
    "    return (1/(np.sqrt(2 * np.pi) * s)) * np.exp(-(x-m)**2/(2*s**2))\n",
    "\n",
    "def y_wrt_m(x,a,m,s):\n",
    "    return  (1/(np.sqrt(2 * np.pi) * s)) * np.exp(-(x-m)**2/(2*s**2))*(a)*(x-m)/(s**2)\n",
    "\n",
    "def y_wrt_s(x,a,m,s):\n",
    "    return  (1/(np.sqrt(2 * np.pi) * s)) * np.exp(-(x-m)**2/(2*s**2))*(a)*(-1/s ) +(1/(np.sqrt(2 * np.pi) * s)) * np.exp(-(x-m)**2/(2*s**2))*(a)*((x-m)**2)/(s**3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(x,a,m,s,y):\n",
    "    return make_gaussian(x,a,m,s)-y\n",
    "    \n",
    "def jacobian(x,a,m,s):\n",
    "    return np.c_[y_wrt_a(x,a,m,s),y_wrt_m(x,a,m,s),y_wrt_s(x,a,m,s)]\n",
    "\n",
    "\n",
    "def gradient_descent(x_obs, y_obs, a, m, s, no_of_itr=no_of_itr, alpha=gd_learning_rate, tolerance=tolerance):\n",
    "    W = [a, m, s]\n",
    "    for _ in range(int(no_of_itr)):\n",
    "        # jac=np.c_[y_wrt_a(x_obs,a,m,s),y_wrt_m(x_obs,a,m,s),y_wrt_s(x_obs,a,m,s)]\n",
    "        jac = jacobian(x_obs, W[0], W[1], W[2])\n",
    "        resi = residual(x_obs, W[0], W[1], W[2], y_obs)\n",
    "        W = W - alpha * jac.T @ resi\n",
    "        R = np.sqrt((resi**2).sum() / no_of_obs)\n",
    "        if R < tolerance:\n",
    "            break\n",
    "\n",
    "    return W,R\n",
    "    \n",
    "W,R=gradient_descent(x_obs,y_obs,initial_amp,initia_mean, initial_sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_resultant_graph(x_obs, y_obs, W):\n",
    "    y_est = make_gaussian(x_obs, W[0], W[1], W[2])\n",
    "    plt.clf()\n",
    "    plt.plot(x_obs,y_obs)\n",
    "    plt.plot(x_obs,y_est)\n",
    "    plt.xlabel(\"X\"),plt.ylabel(\"Y\")\n",
    "    plt.legend([\"Ground Truth\",\"Predicted\"])\n",
    "    plt.show()\n",
    "\n",
    "plot_resultant_graph(x_obs, y_obs, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying iterations\n",
    "itr_list= np.arange(100, 1300, 25)\n",
    "errors=[]\n",
    "print(\"Varying iterations, learning rate= \",\n",
    "      gd_learning_rate, \", tolerance=\", tolerance)\n",
    "\n",
    "for itr in itr_list:\n",
    "    W,R=gradient_descent(x_obs,y_obs,initial_amp,initia_mean,initial_sd,itr)\n",
    "    errors.append(R)\n",
    "\n",
    "    if itr %300 == 0:\n",
    "        y_est= make_gaussian(x_obs, W[0], W[1], W[2])\n",
    "        plt.clf()\n",
    "        plt.plot(x_obs,y_obs)\n",
    "        plt.plot(x_obs,y_est)\n",
    "        plt.xlabel(\"X\"),plt.ylabel(\"Y\")\n",
    "        plt.legend([\"Ground Truth\",\"Predicted\"])\n",
    "        plt.title(\"No of iterations: \" + str(itr))\n",
    "        plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(itr_list,errors)\n",
    "plt.xlabel(\"iteratoins\")\n",
    "plt.ylabel(\"R\")\n",
    "plt.title(\"Residual vs Iterations\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for learning rates\n",
    "learning_rates_list = np.geomspace(1e-4, 1500, 100).round(5)\n",
    "errors=[]\n",
    "print(\"Varying learning rate, max_iterations= \", no_of_itr, \", tolerance= \", tolerance)\n",
    "\n",
    "for lr in learning_rates_list:\n",
    "    W, R = gradient_descent(x_obs, y_obs, initial_amp,\n",
    "                            initia_mean, initial_sd, no_of_itr, lr, tolerance)\n",
    "    errors.append(R)\n",
    "\n",
    "    if len(errors) % 20 == 0:\n",
    "        y_est = make_gaussian(x_obs, W[0], W[1], W[2])\n",
    "        plt.clf()\n",
    "        plt.plot(x_obs,y_obs)\n",
    "        plt.plot(x_obs,y_est)\n",
    "        plt.xlabel(\"X\"),plt.ylabel(\"Y\")\n",
    "        plt.legend([\"Ground Truth\",\"Final Estimate\"])\n",
    "        plt.title(\"Learning rate: \" + str(lr))\n",
    "        plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(learning_rates_list,errors)\n",
    "plt.xlabel(\"learning rates\")\n",
    "plt.ylabel(\"R\")\n",
    "plt.title([\"Error vs learning rates\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying tolerance\n",
    "tol_list= np.geomspace(1e-15, 3, 20)\n",
    "errors=[]\n",
    "max_iter = 10000\n",
    "\n",
    "print(\"Varying tolerance, learning rate= \",\n",
    "      gd_learning_rate, \", max_iterations= \", max_iter)\n",
    "\n",
    "for tol in tol_list:\n",
    "    W, R = gradient_descent(x_obs, y_obs, initial_amp, initia_mean, initial_sd,\n",
    "                            max_iter, gd_learning_rate, tol)\n",
    "    errors.append(R)\n",
    "\n",
    "    if len(errors) % 5 == 0:\n",
    "        y_est=make_gaussian(x_obs, W[0], W[1], W[2])\n",
    "\n",
    "        plt.clf()\n",
    "        plt.plot(x_obs,y_obs)\n",
    "        plt.plot(x_obs,y_est)\n",
    "        plt.xlabel(\"X\"),plt.ylabel(\"Y\")\n",
    "        plt.title(\"Tolerance = \" + str(tol))\n",
    "        plt.legend([\"Ground Truth\",\"Predicted\"])\n",
    "        plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(tol_list,errors)\n",
    "plt.xlabel(\"tol_list\"),plt.ylabel(\"R\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(update_fn):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Another Non-Linear function\n",
    "Now that you've got the hang of computing the jacobian matrix for a non-linear function via the aid of an example, try to compute the jacobian of a secondary gaussian function by carrying out steps similar to what has been shown above. The function is plotted below:\n",
    "<img src='./helpers/non_linear.png' alt=drawing width=500 height=600>\n",
    "Using the computed jacobian, optimise for the four parameters using gradient descent, where the parameters to be estimated are: \n",
    "\n",
    "$p_1$ = 2,  $p_2$ = 8,  $p_3$ = 4,  $p_4$ = 8. \n",
    "\n",
    "Do this for $x_{obs} = np.linspace(-20,30, num\\_obs)$,\n",
    "where $num\\_obs$ is 50.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers.func import make_non_linear\n",
    "\n",
    "def make_non_linear(x, p1, p2, p3, p4):\n",
    "    y = p1 * np.exp(-x / p2) + p3 * np.sin(x / p4)\n",
    "    return y\n",
    "\n",
    "no_of_obs = 50\n",
    "p1, p2, p3, p4 = 2, 8 ,4, 8\n",
    "init_p1, init_p2, init_p3, init_p4 = 3, 6, 5, 7.5  # taken randomly\n",
    "\n",
    "x_obs = np.linspace(-20, 30, no_of_obs)\n",
    "y_obs = make_non_linear(x_obs, p1, p2, p3, p4)\n",
    "\n",
    "def plot_init_state():\n",
    "    y_est = make_non_linear(x_obs, init_p1, init_p2, init_p3, init_p4)\n",
    "    plt.plot(x_obs,y_obs)\n",
    "    plt.plot(x_obs,y_est)\n",
    "    plt.xlabel(\"X\"),plt.ylabel(\"Y\")\n",
    "    plt.legend([\"Ground Truth\",\"Initial Guess\"])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_init_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining partial derivatives\n",
    " \n",
    "def y_wrt_p1(x,p1,p2,p3,p4):\n",
    "    return  np.exp(-1*x / p2) \n",
    "\n",
    "def y_wrt_p2(x,p1,p2,p3,p4):\n",
    "    return p1 * np.exp(-1*x / p2) * x * (1/(p2**2))\n",
    "\n",
    "def y_wrt_p3(x,p1,p2,p3,p4):\n",
    "    return  np.sin(x / p4)\n",
    "\n",
    "def y_wrt_p4(x,p1,p2,p3,p4):\n",
    "    return  p3 * np.cos(x / p4) * (-1/(p4**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual2(x,p1,p2,p3,p4,y):\n",
    "    return make_non_linear(x,p1,p2,p3,p4)-y\n",
    "\n",
    "def jacobian2(x,p1,p2,p3,p4):\n",
    "    return np.c_[y_wrt_p1(x,p1,p2,p3,p4),y_wrt_p2(x,p1,p2,p3,p4),y_wrt_p3(x,p1,p2,p3,p4),y_wrt_p4(x,p1,p2,p3,p4)]\n",
    "\n",
    "\n",
    "def gradient_descent2(x_obs, y_obs, p1, p2, p3, p4, no_of_itr=no_of_itr2, alpha=nonl_learning_rate, tolerance=tolerance2):\n",
    "    W = [p1, p2, p3, p4]\n",
    "    for _ in range(int(no_of_itr)):\n",
    "        jac = jacobian2(x_obs, W[0], W[1], W[2], W[3])\n",
    "        resi = residual2(x_obs, W[0], W[1], W[2], W[3], y_obs)\n",
    "        W = W - alpha * jac.T @ resi\n",
    "        R = np.sqrt((resi**2).sum())/no_of_obs\n",
    "\n",
    "        if R < tolerance:\n",
    "            break\n",
    "        \n",
    "    return W,R\n",
    "    \n",
    "\n",
    "W, R = gradient_descent2(x_obs, y_obs, init_p1, init_p2, init_p3, init_p4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est= make_non_linear(x_obs, W[0], W[1], W[2],W[3])\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_obs,y_obs)\n",
    "plt.plot(x_obs,y_est)\n",
    "plt.xlabel(\"X\"),plt.ylabel(\"Y\")\n",
    "plt.legend([\"Ground Truth\",\"Predicted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Different Optimizers\n",
    "\n",
    "Replace gradient descent with Gauss-Newton and Levenberg Marquardt algorithms and repeat question 1.1. \n",
    "\n",
    "To quickly recap, Gauss-Newton and Levenberg Marquardt are alternate update rules to the standard gradient descent. Gauss Newton updates work as:\n",
    "\n",
    "$$\\delta x = -(J^TJ)^{-1}J^Tf(x)$$\n",
    "\n",
    "Levenberg Marquardt lies somewhere between Gauss Newton and Gradient Descent algorithms by blending the two formulations. As a result, when at a steep cliff, LM takes small steps to avoid overshooting, and when at a gentle slope, LM takes bigger steps:\n",
    "\n",
    "\n",
    "$$\\delta x = -(J^TJ + \\lambda I)^{-1}J^Tf(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "   * 1. How does the choice of initial estimate and learning rate affect convergence? Observations and analysis from repeated runs with modified hyperparameters will suffice.\n",
    "   * 2. Do you notice any difference between the three optimizers? Why do you think that is? (If you are unable to see a clear trend, what would you expect in general based on what you know about them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_range=3\n",
    "lm_range=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_newton(x_obs,y_obs,a,m,s):\n",
    "    W=np.array([a,m,s],dtype=np.double)\n",
    "    for _ in range(gn_range):\n",
    "        J=jacobian(x_obs,W[0],W[1],W[2])\n",
    "        R=residual(x_obs,W[0],W[1],W[2],y_obs)\n",
    "        W = W-np.linalg.pinv(J.T @ J)@J.T@R\n",
    "    return W[0],W[1],W[2]\n",
    "\n",
    "a,m,s=gauss_newton(x_obs,y_obs,10,13,19.12)\n",
    "\n",
    "y_obs,y_est=[],[]\n",
    "\n",
    "for _ in x_obs:\n",
    "    y_obs.append(make_gaussian(_,10,0,20))\n",
    "    y_est.append(make_gaussian(_,a,m,s))\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_obs,y_obs)\n",
    "plt.plot(x_obs,y_est)\n",
    "plt.xlabel(\"X\"),plt.ylabel(\"Y\")\n",
    "plt.legend([\"Ground Truth\",\"Predicted\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer for part 1\n",
    "# choice of initial estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LM\n",
    "def error(amp, mu, sigma, x, y):\n",
    "    return np.linalg.norm(residual(amp, mu, sigma, x, y)) ** 2\n",
    "\n",
    "# return \n",
    "lmda=1\n",
    "\n",
    "def lm(x_obs,y_obs,a,m,s,lmda):\n",
    "    W=np.array([a,m,s],dtype=np.double)\n",
    "    errors = [error(W[0],W[1],W[2], x_obs, y_obs), ]\n",
    "    for _ in range(lm_range):\n",
    "        new_error = error(W[0],W[1],W[2], x_obs, y_obs)\n",
    "\n",
    "        J=jacobian(x_obs,W[0],W[1],W[2])\n",
    "        R=residual(x_obs,W[0],W[1],W[2],y_obs)\n",
    "        W=W-np.linalg.pinv((J.T @ J) + (lmda * np.eye(J.shape[1]))) @ J.T @ R\n",
    "        # print(W)\n",
    "        if len(errors) > 0:\n",
    "            if new_error > errors[-1]:\n",
    "                lmda = lmda * 2\n",
    "            else:\n",
    "                lmda = lmda / 3\n",
    "        errors.append(new_error)\n",
    "        \n",
    "    return W[0],W[1],W[2]\n",
    "\n",
    "a,m,s=lm(x_obs,y_obs,10,13,19.12,lmda)\n",
    "\n",
    "y_obs,y_est=[],[]\n",
    "\n",
    "for _ in x_obs:\n",
    "    y_obs.append(make_gaussian(_,10,0,20))\n",
    "    y_est.append(make_gaussian(_,a,m,s))\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_obs,y_obs)\n",
    "plt.plot(x_obs,y_est)\n",
    "plt.xlabel(\"X\"),plt.ylabel(\"Y\")\n",
    "plt.legend([\"Ground Truth\",\"Predicted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1. \n",
    "\n",
    "\n",
    "Answer 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iterative Closest Point\n",
    "\n",
    "In this subsection, we will code the Iterative Closest Point algorithm to find the alignment between two point clouds without known correspondences. The point cloud that you will be using is the same as the one that you used in Assignment 1.\n",
    "\n",
    "## 2.1: Procrustes alignment\n",
    "\n",
    "1. Write a function that takes two point clouds as input wherein the corresponding points between the two point clouds are located at the same index and returns the transformation matrix between them.\n",
    "2. Use the bunny point cloud and perform the procrustes alignment between the two bunnies. Compute the absolute alignment error after aligning the two bunnies.\n",
    "3. Make sure your code is modular as we will use this function in the next sub-part.\n",
    "4. Prove mathematically why the Procrustes alignment gives the best aligning transform between point clouds with known correspondences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: ICP alignment\n",
    "\n",
    "1. Write a function that takes two point clouds as input without known correspondences and perform the iterative closest point algorithm.\n",
    "2. Perform the ICP alignment between the two bunnies and plot their individual coordinate frames as done in class.\n",
    "3. Does ICP always give the correct alignment? Why or Why not?\n",
    "4. What are other variants of ICP and why are they helpful (you can look at point to plane ICP)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b74ed72cb0c37fa87a4743ec715b1c69c86022b805e1f8dea626b305bf9ed7d9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('mr': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
