{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment - 2: Data Representation and Point Cloud Operations\n",
    "\n",
    "Team Name: Sentinels \n",
    "\n",
    "Roll number: 2019101054 2019111002"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instructions\n",
    "\n",
    "- Code must be written in Python in Jupyter Notebooks. We highly recommend using anaconda distribution or at the minimum, virtual environments for this assignment.\n",
    "- Save all your results in ```results/<question_number>/<sub_topic_number>/```\n",
    "- The **References** section provides you with important resources to solve the assignment.\n",
    "- Make sure your code is modular since you may need to reuse parts for future assignments.\n",
    "- Answer the descriptive questions in your own words with context & clarity. Do not copy answers from online resources or lecture notes.\n",
    "- The **deadline** for this assignment is on 26/09/2021 at 11:55pm. Please note that there will be no extensions.\n",
    "- Plagiarism is **strictly prohibited**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submission Instructions\n",
    "\n",
    "1. Make sure your code runs without any errors after reinitializing the kernel and removing all saved variables.\n",
    "2. After completing your code and saving your results, zip the folder with name as ``Team_<team_name>_MR2021_Assignment_<assignment_number>.zip``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "from IPython.display import display"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to types of Transformations and Homogeneous coordinates\n",
    "\n",
    "In robotics applications, it is inevitable to keep track of the frames of multiple objects/worlds. These frames can be transformations from one coordinate frame to the other. **Homogeneous coordinates** help in keeping track of various coordinate frames and allow performing composition of various transforms. We will first try to understand between types of transformations and their invariant properties.\n",
    "1. What is the difference between Affine, Similarity, and Euclidean transform? What are the invariant properities of each type of transform?\n",
    "2. Watch this [video](https://www.youtube.com/watch?v=PvEl63t-opM) to briefly understand homogeneous coordinates. What are points at infinity? What type of transformation can you apply to transform a point from infinity to a point that is not at infinity? \n",
    "3. Using homogeneous coordinates we can represent different types of transformation as point transforms vs. frame transforms. Concatenation of transforms (whether you post multiply transformation matrices or pre-multiply transformation matrices) depends on the problem and how you are viewing it. Try to understand the difference between frame vs. point transformations from this [video](https://youtu.be/Za7Sdegf8m8?t=1834). Let's assume that our camera and world frames are coinciding with each other. We need to estimate the camera to world **frame** transformation matrix after applying the transformations defined below in terms of $T_i$.We apply **frame** transform to move the camera in the world in the following order:\n",
    "    1. $T_1$ from the camera coordinate frame.\n",
    "    2. $T_2$ from the world coordinate frame.\n",
    "    3. $T_3$ from the world coordinate frame.\n",
    "    4. $T_4$ from the camera coordinate frame.\n",
    "    5. $T_5$ from the camera coordinate frame.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer1. \n",
    "**Affine transformation** (linear transformation and translation) has 6 DOF in 2d (12 DOF {3 rotation 3 translation 3 scaling and 3 shear} in 3d) and its *invariant properties* are parallelism, the ratio of areas, the ratio of lengths on collinear or parallel lines (eg midpoints), a linear combination of vectors, and line at infinity.  \n",
    "**Similarity transform** (rotation, translation, and scale) has 4 DOF  in 2d  (7 DOF {3 rotation 3 translation 1 scaling} in 3d).  \n",
    "and its invariant properties include the ratio of lengths and angle and the isotropic scaling\n",
    "**Euclidean transform** (rotation, translation, or reflection) has 3 DOF in 2d  (6 DOF {3 rotation  3 translation} in 3d) and invariant properties include length and area.  \n",
    "\n",
    "Detailed Explanation (for 2D):  \n",
    "**Affine transformations** are made up of a nonsingular linear transformation (rotation, scaling, etc ) plus a translation.  \n",
    "    Properties:\n",
    "    1. Origin does not necessarily match the origin.  \n",
    "    It only stretches, reflects, rotates(for example diagonal matrix or orthogonal matrix), or shears(matrix with off-diagonal elements) a vector(the same applies to many vectors/a matrices).  \n",
    "    2. Lines map to lines: We can see in the above transformation that the lines in the original pictures are still lines after the transformation, no bending.  \n",
    "    3.Parallel lines  remain parallel after the transformation.  \n",
    "    4.Ratios of lengths along lines preserved (midpoints preserved).  \n",
    "    In general, an affine transformation is a composition of rotations, translations, dilations, and shears.While an affine transformation preserves proportions on lines, it does not necessarily preserve angles or lengths.  \n",
    "    Any triangle can be transformed into any other by an affine transformation, so all triangles are affine and, in this sense, \n",
    "    affine is a generalization of congruent and similar.\n",
    "\n",
    "**Similarity transform** is an isometry composed with an isotropic scaling.  \n",
    "    Properties:  \n",
    "    1. It preserves the shape  \n",
    "    2. It has 4 degrees of freedom in 2d one for scaling, one for rotation, and two for translation  \n",
    "\n",
    "\n",
    "**Euclidean transformations** preserve the invariant size of the figure, they change only the figure position in the space.  \n",
    "Properties:  \n",
    "    1. Euclidean transformations preserve the distance (length), the distance between the points on the figure (the process is called translation).  \n",
    "    2. Euclidean transformations preserve angle measures. (ex. rotation of 180degrees with the center of rotation at the origin).  \n",
    "    3. Euclidean transformations preserve betweenness, which means like c is a point between a and b the dist from a to c + dist from c to b = dist from a to b.  \n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer 2.  \n",
    "**Points at infinity**: By setting the last coordinate in homogeneous coordinate representation to zero we can express the \n",
    "points that are infinitely far away with finite coordinates. What it basically means is that the points in the form of [x, y, 0] in homogeneous coordinates are points at infinity. It is as if we divide the x and y with 0 if we want to go back to the Euclidean world, we get points infinitely far away, and also we still maintain the directions to the points. Even if the point is infinitely far away we can still estimate its direction.  \n",
    "\n",
    "\n",
    "A point from infinity can be transformed to the point that is not at infinity with the help of projective transformation \n",
    "that has non-zero elements in the last row.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer 3  \n",
    "Required transformation  is $(T_3 T_2 T_i T_1 T_4 T_5)^{-1}$ = $T_5^-1 * T_4^-1 * T_1^-1 * T_i^-1* T_2 ^-1 * T_3^-1$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualise the Data\n",
    "\n",
    "Point clouds are a collection of points that represent a 3D shape or feature. Each point has its own set of X, Y and Z coordinates and in some cases additional attributes. A popular way to obtain this is by photogrammetry, though here we will use LiDAR data.\n",
    "\n",
    "LiDAR is a remote sensing process which collects measurements used to create 3D models and maps of objects and environments. Using ultraviolet, visible, or near-infrared light, LiDAR gauges spatial relationships and shapes by measuring the time it takes for signals to bounce off objects and return to the scanner.\n",
    "\n",
    "1. Download the data from [here](https://iiitaphyd-my.sharepoint.com/:f:/g/personal/venkata_surya_students_iiit_ac_in/EnYAMaTVIhJItzKYqtahE30BRKB6p6UfHN3TyJzvo6Mw0g?e=PegWds). It contains the LIDAR sensor output and odometry information per frame.\n",
    "\n",
    "    The .bin files contain the 3D point cloud captured by the LIDAR in this format - x, y, z, and reflectance. \n",
    "\n",
    "    The odometry information is given in the `odometry.txt` file, which is a 12 element vector. Reshape each of the first 77 rows to a 3x4 matrix to obtain the pose.\n",
    "    \n",
    "\n",
    "2. Obtain the point cloud from this and visualise for 1-2 frames."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# reading and generating transformations for:  i_th cam frame to world frame\n",
    "\n",
    "def generate_odo_T():\n",
    "    with open(\"data/odometry.txt\") as f:\n",
    "        data = f.readlines()[:77]\n",
    "    data = [list(map(float, line.split())) for line in data]\n",
    "    odo= np.array(data).reshape(-1, 3,4)\n",
    "    cam_to_world = []\n",
    "    for rot_mat in odo:\n",
    "        cam_to_world.append(np.vstack((rot_mat, [0,0,0,1])))\n",
    "    return cam_to_world\n",
    "\n",
    "odo_mat = generate_odo_T()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/odometry.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-896e63847b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcam_to_world\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0modo_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_odo_T\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-896e63847b3d>\u001b[0m in \u001b[0;36mgenerate_odo_T\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_odo_T\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/odometry.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/odometry.txt'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# vizualizing some frames of LIDAR output\n",
    "def get_lidar_frame(i, homo=False):\n",
    "    bindata = np.fromfile(\"./data/LiDAR/\" + str(i+10).zfill(6)+\".bin\",\n",
    "                          dtype=np.float32)\n",
    "    points_in_lidar = bindata.reshape(-1, 4)\n",
    "    if homo:\n",
    "        points_in_lidar[:, 3] = 1\n",
    "        return points_in_lidar\n",
    "    \n",
    "    return points_in_lidar[:, :3]\n",
    "\n",
    "\n",
    "for i in [10,33]:\n",
    "    pcd = o3d.geometry.PointCloud(points= o3d.utility.Vector3dVector(\n",
    "        get_lidar_frame(i, homo=False)\n",
    "    )).voxel_down_sample(1)\n",
    "    o3d.visualization.draw_geometries([pcd])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transform \n",
    "\n",
    "The point cloud obtained is with respect to the LiDAR frame. The poses however, are in the camera frame. If we want to combine the point clouds from various frames, we need to bring them to the camera frame. \n",
    "\n",
    "1. Refer to the image below and apply the required transformation to the point cloud. \n",
    "\n",
    "2. Then, register all point clouds into a common reference frame and visualise it (Open3D). It is helpful to use homogeneous coordinates to keep track of the different frames.\n",
    "\n",
    "3. Write a function to transform the registered point cloud from the world to the $i^{th}$ camera frame, wherein $i$ is the input to the function.\n",
    "\n",
    "4. \\[Bonus\\] Move around in the registered point cloud using arrow keys like you would do in a game. For this you will have to regularly transform the entire registered world to your current camera frame and visualize repeatedly. You may choose to avoid visualizing points that are behind the camera in this case as they are not visible from the scene. You may also visualize points at a max depth to make the process easier.\n",
    "\n",
    "![](./img/transform.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#registering all points in one frame\n",
    "all_points = None\n",
    "T_lidar_to_cam = np.array(\n",
    "    [[0, -1, 0, 0], [0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 0, 1]\n",
    "])\n",
    "T_init_cam_to_world = np.array([\n",
    "    [0, 0, 1, 0], [-1, 0, 0, 0], [0, -1, 0, 0], [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "for i in range(77):\n",
    "    points_in_lidar = get_lidar_frame(i, homo=True)[::5]\n",
    "    points_in_cam = points_in_lidar @ T_lidar_to_cam.T\n",
    "    points_in_init_cam = points_in_cam @ odo_mat[i].T\n",
    "    points_in_world = points_in_init_cam @ T_init_cam_to_world.T\n",
    "    if all_points is None:\n",
    "        all_points = points_in_world\n",
    "    else:\n",
    "        all_points = np.concatenate((all_points, points_in_world))\n",
    "\n",
    "# vizualizing all points in one frame\n",
    "pcd = o3d.geometry.PointCloud(points=o3d.utility.Vector3dVector((all_points)[:, :3]))\n",
    "print(len(pcd.points))\n",
    "pcd = pcd.voxel_down_sample(2)\n",
    "print(len(pcd.points))\n",
    "o3d.visualization.draw_geometries([pcd])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1882721\n",
      "8024\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# function to convert pcd to i_th cam frame\n",
    "def to_cam_frame(points, i:int)->o3d.geometry.PointCloud:\n",
    "    points = points @ T_init_cam_to_world\n",
    "    points = points @ odo_mat[i]\n",
    "    return o3d.geometry.PointCloud(points=o3d.utility.Vector3dVector(\n",
    "        points[:, :3]\n",
    "    )).voxel_down_sample(2)\n",
    "\n",
    "\n",
    "# vizualizing merged point cloud in a camera frame\n",
    "newpcd = to_cam_frame(all_points, 2)\n",
    "o3d.visualization.draw_geometries([newpcd])\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'o3d' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d64f56183ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# function to convert pcd to i_th cam frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mto_cam_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mo3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPointCloud\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mT_init_cam_to_world\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0modo_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return o3d.geometry.PointCloud(points=o3d.utility.Vector3dVector(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'o3d' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bonus\n",
    "move around the point cloud using the WASD keys in open3d scene vizualizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_key_callbacks():\n",
    "    def move(vis, tr=[20, 0, 0]):\n",
    "        \n",
    "        ctr = vis.get_view_control()\n",
    "        ctr.translate(tr[0], tr[1])\n",
    "        return False\n",
    "\n",
    "    return {\n",
    "        ord(\"U\"): lambda vis: move(vis, [0, 0, 15]),\n",
    "        ord(\"I\"): lambda vis: move(vis, [0, 0, -15]),\n",
    "        ord(\"A\"): lambda vis: move(vis, [-15, 0, 0]),\n",
    "        ord(\"D\"): lambda vis: move(vis, [15, 0, 0]),\n",
    "        ord(\"W\"): lambda vis: move(vis, [0, -15, 0]),\n",
    "        ord(\"S\"): lambda vis: move(vis, [0, 15, 0]),\n",
    "    }\n",
    "\n",
    "\n",
    "o3d.visualization.draw_geometries_with_key_callbacks(\n",
    "    [pcd], get_key_callbacks())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Occupancy Map"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Occupancy grid maps are discrete fine grain grid maps. These maps can be either 2-D or 3-D. Each cell in the occupancy grid map contains information on the physical objects present in the corresponding space. Since these maps shed light on what parts of the environment are occupied, and what is not, they are really useful for path planning and navigation.\n",
    "\n",
    "Occupancy grid maps are probabilistic in nature due to noisy measurements. Each cell can have three states: Occupied, unoccupied, and unknown. For the purpose of this assignment, you can ignore the unknown and work in a binary setting where 1 is occupied and 0 is unoccupied."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. The task here is to create an occupancy map for each LiDAR scan. You do not need to apply bayesian update rules here, just keep it simple. \n",
    "\n",
    "2. Now, using the *registered* point cloud, generate occupancy maps for each frame. What difference do you expect to see between the two methods?\n",
    "\n",
    "You can mark a cell as occupied based on a threshold of how many different z values are there for a particular (x,y) cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def coords_to_ocMap(points, threshold = 1):\n",
    "    points = points[:, :3]      #if homo, now not\n",
    "    points = np.array(points, dtype=np.int)\n",
    "    df = pd.DataFrame(points, columns=['x','y','z']).drop_duplicates()\n",
    "    # print(df.describe())\n",
    "    df = df.groupby([\"x\",\"y\"]).count()\n",
    "    # print(df.head())\n",
    "    # print(df.describe())\n",
    "    xy_coords = np.array([*df.index.values]) + 200\n",
    "    z_counts = np.array(df['z'])\n",
    "    \n",
    "    occMap  = np.zeros((400,400), dtype=np.uint8)\n",
    "    for (x,y), zcnt in zip(xy_coords, z_counts):\n",
    "        occMap[x][y] = 255 * int(zcnt > threshold)\n",
    "    return occMap\n",
    "\n",
    "def save_oc_map(ocmap, save_path:str):\n",
    "    Image.fromarray(ocmap, 'L').save(save_path)\n",
    "   \n",
    "def display_ocmap(occMap, r = None):\n",
    "    # plt.imshow(occMap, \"gray\")\n",
    "    display(Image.fromarray(occMap, 'L'))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# displaying an frame\n",
    "oc_map = coords_to_ocMap(get_lidar_frame(0))\n",
    "display_ocmap(oc_map)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=400x400 at 0x7F36FC6A0430>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# saving all individual lidar scans as occ map\n",
    "# in results/XXXXXX.png\n",
    "for i in range(77):\n",
    "    oc_map = coords_to_ocMap(get_lidar_frame(i))\n",
    "    save_oc_map(oc_map, \"results/\" + str(i).zfill(6) + \".png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# occ map for all frames in one\n",
    "# using all_points calculated in previous question \n",
    "oc_map = coords_to_ocMap(all_points[:, :3])\n",
    "display_ocmap(oc_map)\n",
    "save_oc_map(oc_map, \"results/combined.png\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=400x400 at 0x7F36FC6A05E0>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('mr': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "6a4bba13c6c520c337f2cf866ee6e68ac6acf36f7ecf67f31b6e792885da0c38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}