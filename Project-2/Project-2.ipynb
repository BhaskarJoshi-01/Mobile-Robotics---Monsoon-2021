{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is project-2 which will get you familiar with Structure from Motion and a rudimentary stereo SLAM pipeline. This notebook is not meant for coding, refer to the notebooks in `questions/` for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-heather",
   "metadata": {},
   "source": [
    "**Team Name**: Sentinels\n",
    "\n",
    "**Team Members/Rollnumbers**: 2019101054 2019111002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions:\n",
    "1. Zip the entire folder, not just this notebook, and submit on Moodle. \n",
    "2. Be sure to solve all questions that are in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment will take around 10-12 hours, so be sure to split the work equally. A recommendation to split the work would be to have one person start with stereo, the other with bundle adjustment, and then both switch to completing the PnP and ICP parts. This is because question-1 and 2 are separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Stereo, PnP, and ICP\n",
    "\n",
    "Time: ~6-7 Hours\n",
    "\n",
    "This question has been designed to test your understanding of the concepts that create a SLAM system, from creating a front-end using Stereo Matching to a backend where you optimise to get your odometry to stitch the pointclouds obtained from the front end to build a map of the environment.\n",
    "\n",
    "The dataset has been provided in `data/`. `img2` and `img3` correspond to the left and right camera respectively. `poses.txt` has the pose information for all frames. `calib.txt` has camera calibration information.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Bundle Adjustment\n",
    "\n",
    "Time: ~2-3 Hours\n",
    "\n",
    "This question will walk you through a simple implementation of Bundle Adjustment and run it on a a small dataset. Most of the weightage for this is in the theory part, where you have to explain the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the paper 'Building Rome in a Day' and briefly write about the fundamental idea behind the problem and solution. No need to be verbose, just write about the challenge with the task and how the pipeline is implemented (do not include details about performance/parallelization).\n",
    "\n",
    "2. How is this task different from a SLAM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer1. \n",
    "The fundamental idea of the problem beign proposed is that it is uncalibrated , unstructured , large and fast image matching and 3d reconstruction of a large dataset of images and  found by searching for Rome city on Internet sharing photo sites. The images are uncalibrated and without any gps data ,just matching similar images and construct 3d map with the help of matched images. Also the results are generated within a day in fast and effiecient manner. \n",
    "Results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day.\n",
    "- Firstly they used SIFT for the extraction of image features\n",
    "- Used Nearest-neighbour search for getting the correspondences for pair of images with those extracted features.\n",
    "- Then a graph was constructed having images as vertices and there was an edge for similar images only , for this they randomly proposed an edge between vertices and only kept those which satisfy the similarity threshold.\n",
    "- The previous portion is based on entire picture similarity, in which they treated images as documents and mapped characteristics to 'visual words'.\n",
    "- Then they employed the TF-IDF-based document retrieval system to find the best/most comparable k1 + k2 photos to the current image under consideration. \n",
    "- Following that, they verified just a subset of the best k1 suggested edges.\n",
    "- Further imporvement to this was added by decreasing the no. of connected components and verifying edges from remaining k2 suggestions\n",
    "- At last the query expansion was applied where all vertices by distance of 2 edges are connected by direct edge and new edge is verified before insertion.\n",
    "- Now the track of features was generated \n",
    "- For making the above mentioned change , they create a graph with all of the features from all of the photos as vertices and connections connecting matching/similar features.(connected corresponds to tracks for specific 3d points)\n",
    "- Then they selected a subset of images to eliminate duplicate images in order to minimise the number of images for quicker processing in SFM without sacrificing accuracy.\n",
    "- To acquire the 3D points and postures, they use custom SFM with bundle adjustment and the LM optimizer.\n",
    "- At last they cluster the pictures and employ a multi-view stereo algorithm to enhance estimate of the sparse 3D points derived from SFM.\n",
    "\n",
    "Answer2.\n",
    "SLAM uses images or poses from the same sensor but in the mentioned case, we had images from different sensors whose calibrations are also not known.\n",
    "In comparison to SLAM, the data is quite  scattered in above case.\n",
    "The environment that we talk about in SLAM is small as compared to the above mentioned case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "## 1. SfM pipeline (`6 mark`)\n",
    "\n",
    "To get the context of below questions, take a look at the code above: The same questions have been asked at different places above as comments in the code.\n",
    "\n",
    "1. `0.5 mark` **Basics** - How do we know this (`camera_ind`) information in practical setting? In other words, how do we know observations in `points_2d` belong to which camera. Explain. \n",
    "    - Ans-1 - Basics: In practical setting  we can correlate each image with a camera pose. As a result, there is a correspondence between the picture and the camera pose. We may extend this correspondence to the image coordinates because the points recorded in points 2d are from the image. Typically, these images are obtained through a feature extractor such as SIFT.\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "2. `0.5 mark` **Basics** - How do we know this (`point_ind`) information in practical setting?  In other words, how do we know observations in `points_2d` belong to which 3D point. Explain.\n",
    "    - Ans-2 - Basics: In practical settings, we know that images in different perspectives are captured at different positions. We can use SIFT/SURF or ORB to determine corresponding points between two photos using this information. After this  8-point algorithm estimates the Fundamental Matrix, from which we may obtain the Essential Matrix and decompose it into R,t, and K. To obtain 3D coordinates, we can now triangulate or backproject points to the world. As a result, we now have our 2D-3D correspondences.\n",
    "\n",
    "\n",
    "    \n",
    "3. `0.5 mark` **Transformations** - `rotate()` function: Why do we use the rodriquez formula? How is this representation different from the standard 3x3 Rotation matrix, why do we use this instead?\n",
    "    - Ans-3 - Transformations: Rodrigue's formula defines rotation of a point $P$ as $P$ being rotated by an angle $theta$ about a fixed axis $n$. \n",
    "    This is distinct from our usual Rotation matrix, in which we convert a point by multiplying it by a 3x3 rotation matrix (R). \n",
    "    So, in the instance of Rodrigue, we don't require a 3x3 matrix\n",
    "    We only need minimum three parameters : two for the rotation vector  and one for theta. (the third can be computed from the other two because this vector is a unit vector).\n",
    "    We do this because we have a camera params matrix (3x3) with 9 parameters in this example (including the 3 parameters required for rotation by rodrigues). \n",
    "    We were given a 3x4 projection matrix with a total of 12 parameters in our lesson (this is because it contains R,t and K). \n",
    "    So in all, we have $9*n cameras$ parameters to estimate in this scenario, but in the case of a 3x3 rotation matrix, we have $12*n cameras$ parameters to estimate.\n",
    "    \n",
    "    $$\\mathbf{R}=\\cos \\theta \\mathbf{I}+(1-\\cos \\theta) \\mathbf{n n}^{\\mathrm{T}}+\\sin \\theta \\mathbf{n}^{\\wedge}$$\n",
    "\n",
    "    \n",
    "4. `0.5 mark` **Transformations** - `project()` function: In the `project()` function, would it make any difference if I do translate first, then rotate? Why/why not?\n",
    "    - Ans-4 - Transformations: Yes, there could be a distinction. Translation precedes rotation, implying that we move to a different origin first and then rotate about that origin. When we translate after rotating, we are also moving the rotation centre.\n",
    "    If the translation vector is in the camera frame, the point must first be rotated to the camera frame before being added to the translation vector; otherwise, the translation vector and the point would be in different frames.\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "5. `0.5 mark` **Jacobian** - `bundle_adjustment_sparsity()` function: m above is not \"M*N\" (*2) unlike our lecture notes. Why is that so?\n",
    "    - Ans-5 - Jacobian : As every row in Jacobian corresponds to a residue, we have as many rows as no. of residuals and that is equal to the total no. of 2d points we have as the loss function measures the diff between reporjected 2d points calc from estimated 3d points and estimated parameters and obs 2d points from image.\n",
    "    Now because each 2d point contributes 2 coordinates and not one i.e x and y , so we have twice as many residuals as the number of 2d points.\n",
    "    And it is different from class because in class we have assumed each N point is visible in M images so it leaves us with MN 2d points.Here however we have camera_indices.size 2d points as each element in camera_indices maps 2d point to image so twice as many rows or residuals in jacobian.\n",
    "    \n",
    "6. `2 mark` **Jacobian & Parameters** - `bundle_adjustment_sparsity()` function: \n",
    "    1.  Why are we doing `n_cameras * 9` here instead of `n_cameras * 12`? Recollect: Every individual motion Jacobian was (1*)12 in our lecture notes. \n",
    "        - Ans 6.1: Jacobian & Parameters: We used the homogeneous transformation matrix of dimensions 3x4 in the lecture, which gave us 12 unknowns for each camera. This matrix combines the intrinsics and the extrinsics (rotation and translation) into a single matrix, and we just optimise for it. \n",
    "        In this case, we segregated the various parameters by utilising distinct equations to describe the individual parts. \n",
    "        We employed one equation for the extrinsics, which included three rotation parameters and three translation parameters, and one equation for the intrinsics, which included three parameters (1 focal length + two radial distortion). \n",
    "        We optimised for each of these factors as a separate entity, resulting in just 9 unknowns per camera.   \n",
    "        \n",
    "    2. Ignoring the scale parameters, what was the number of unknown parameters in our lecture notes in terms of `n_cameras` and `n_points`? What is it here in the code? Is it different? If so, what is and why? [Link of notes](https://www.notion.so/Stereo-Structure-from-Motion-9fdd81e4194f4803ac9ba7552df56470).\n",
    "        - Ans 6.2: Jacobian and Parameters: We had `n points * 3 + n cameras * 12` unknowns in the lecture and   we have `n cameras * 9 + n points * 3` unknowns in the code.\n",
    "        we have `n cameras * 9 + n points * 3` unknowns in the code. The primary difference here is the quantity of unknowns associated with the poses/camera frames. \n",
    "        In the lecture, we avoided optimising over the SO(3) manifold by optimising over the projection matrix of dimensions 3x4 and only required to explicitly estimate the 12 projection parameters, which comprised both extrinsics and intrinsics in a single matrix. In contrast, we model the extrinsics and intrinsics independently in this case, employing a rotation vector (3), a translation vector (3), and three extra parameters of focal length (1) and radial distortion (2), totaling nine unknowns for each view of the camera.         \n",
    "            \n",
    "            \n",
    "7. `6 mark` **Sparsity, Residual Vector & Jacobian** - `bundle_adjustment_sparsity()` function: Explain what you understand from above 6 lines of code by coding a simple toy example yourself to illustrate how it is different from what you've learnt in class. ([Coding toy example + elaborating in words]- both are compulsory.) For the toy example, you can take something like 3 points all seen from 3 cameras. (You don't actually have to code much, just need to call the existing function) Write that toy example after this cell\n",
    "    - Ans 6 - Sparsity, Residual Vector & Jacobian: \n",
    "- This code essentially initialises the sparse Jacobian by designating non-zero items as 1 and leaving the remainder as 0.\n",
    "- For each of the two rows of each observation/2D image point, the 9 columns of the corresponding camera parameters and the 3 columns of the corresponding 3D point are set to one.\n",
    "- Because they have no effect on the observation, the remaining values/columns are set to 0.  The next for loop sets 3 cols corresponding to the world point in the ith observation to 1 for all i. Other elements remain 0 and form a sparse structure.\n",
    "- Each column in the Jacobian represents an unknown parameter being optimised, whereas each row represents an observation/equation. Because each observation/2D point has two equations, one for the x and one for the y, we have 2 * num obs rows. Because we have (n cameras * 9 + n points * 3) unknowns, the Jacobian has (n cameras * 9 + n points * 3) columns.\n",
    "- The distinction is that in our lecture notes, we used M for the number of photos and N for the number of points in each image, and we had MN 2D points, each with two coordinates, resulting in 2MN residuals for the Jacobian rows. \n",
    "- m has a distinct size since it contains information about all of the observation made collectively (all 2D points), i.e., 'camera indices.size' 2D points.\n",
    "- In lectures, residuals were arranged first by points and then by pictures, but here they appear in no particular sequence, which has no effect if order is maintained along the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera_idx =  [0 0 0 1 1 1 2 2 2]\n",
      "point_idx =  [0 1 2 0 1 2 0 1 2]\n",
      "Sparse Jacobian is :\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 29)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 27)\t1\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 27)\t1\n",
      "  (2, 28)\t1\n",
      "  (2, 29)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 27)\t1\n",
      "  (3, 28)\t1\n",
      "  (3, 29)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 1)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 3)\t1\n",
      "  (4, 4)\t1\n",
      "  (4, 5)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 7)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 27)\t1\n",
      "  (4, 28)\t1\n",
      "  (4, 29)\t1\n",
      "  (5, 0)\t1\n",
      "  (5, 1)\t1\n",
      "  (5, 2)\t1\n",
      "  (5, 3)\t1\n",
      "  (5, 4)\t1\n",
      "  (5, 5)\t1\n",
      "  (5, 6)\t1\n",
      "  (5, 7)\t1\n",
      "  (5, 8)\t1\n",
      "  (5, 27)\t1\n",
      "  (5, 28)\t1\n",
      "  (5, 29)\t1\n",
      "  (6, 9)\t1\n",
      "  (6, 10)\t1\n",
      "  (6, 11)\t1\n",
      "  (6, 12)\t1\n",
      "  (6, 13)\t1\n",
      "  (6, 14)\t1\n",
      "  (6, 15)\t1\n",
      "  (6, 16)\t1\n",
      "  (6, 17)\t1\n",
      "  (6, 30)\t1\n",
      "  (6, 31)\t1\n",
      "  (6, 32)\t1\n",
      "  (7, 9)\t1\n",
      "  (7, 10)\t1\n",
      "  (7, 11)\t1\n",
      "  (7, 12)\t1\n",
      "  (7, 13)\t1\n",
      "  (7, 14)\t1\n",
      "  (7, 15)\t1\n",
      "  (7, 16)\t1\n",
      "  (7, 17)\t1\n",
      "  (7, 30)\t1\n",
      "  (7, 31)\t1\n",
      "  (7, 32)\t1\n",
      "  (8, 9)\t1\n",
      "  (8, 10)\t1\n",
      "  (8, 11)\t1\n",
      "  (8, 12)\t1\n",
      "  (8, 13)\t1\n",
      "  (8, 14)\t1\n",
      "  (8, 15)\t1\n",
      "  (8, 16)\t1\n",
      "  (8, 17)\t1\n",
      "  (8, 30)\t1\n",
      "  (8, 31)\t1\n",
      "  (8, 32)\t1\n",
      "  (9, 9)\t1\n",
      "  (9, 10)\t1\n",
      "  (9, 11)\t1\n",
      "  (9, 12)\t1\n",
      "  (9, 13)\t1\n",
      "  (9, 14)\t1\n",
      "  (9, 15)\t1\n",
      "  (9, 16)\t1\n",
      "  (9, 17)\t1\n",
      "  (9, 30)\t1\n",
      "  (9, 31)\t1\n",
      "  (9, 32)\t1\n",
      "  (10, 9)\t1\n",
      "  (10, 10)\t1\n",
      "  (10, 11)\t1\n",
      "  (10, 12)\t1\n",
      "  (10, 13)\t1\n",
      "  (10, 14)\t1\n",
      "  (10, 15)\t1\n",
      "  (10, 16)\t1\n",
      "  (10, 17)\t1\n",
      "  (10, 30)\t1\n",
      "  (10, 31)\t1\n",
      "  (10, 32)\t1\n",
      "  (11, 9)\t1\n",
      "  (11, 10)\t1\n",
      "  (11, 11)\t1\n",
      "  (11, 12)\t1\n",
      "  (11, 13)\t1\n",
      "  (11, 14)\t1\n",
      "  (11, 15)\t1\n",
      "  (11, 16)\t1\n",
      "  (11, 17)\t1\n",
      "  (11, 30)\t1\n",
      "  (11, 31)\t1\n",
      "  (11, 32)\t1\n",
      "  (12, 18)\t1\n",
      "  (12, 19)\t1\n",
      "  (12, 20)\t1\n",
      "  (12, 21)\t1\n",
      "  (12, 22)\t1\n",
      "  (12, 23)\t1\n",
      "  (12, 24)\t1\n",
      "  (12, 25)\t1\n",
      "  (12, 26)\t1\n",
      "  (12, 33)\t1\n",
      "  (12, 34)\t1\n",
      "  (12, 35)\t1\n",
      "  (13, 18)\t1\n",
      "  (13, 19)\t1\n",
      "  (13, 20)\t1\n",
      "  (13, 21)\t1\n",
      "  (13, 22)\t1\n",
      "  (13, 23)\t1\n",
      "  (13, 24)\t1\n",
      "  (13, 25)\t1\n",
      "  (13, 26)\t1\n",
      "  (13, 33)\t1\n",
      "  (13, 34)\t1\n",
      "  (13, 35)\t1\n",
      "  (14, 18)\t1\n",
      "  (14, 19)\t1\n",
      "  (14, 20)\t1\n",
      "  (14, 21)\t1\n",
      "  (14, 22)\t1\n",
      "  (14, 23)\t1\n",
      "  (14, 24)\t1\n",
      "  (14, 25)\t1\n",
      "  (14, 26)\t1\n",
      "  (14, 33)\t1\n",
      "  (14, 34)\t1\n",
      "  (14, 35)\t1\n",
      "  (15, 18)\t1\n",
      "  (15, 19)\t1\n",
      "  (15, 20)\t1\n",
      "  (15, 21)\t1\n",
      "  (15, 22)\t1\n",
      "  (15, 23)\t1\n",
      "  (15, 24)\t1\n",
      "  (15, 25)\t1\n",
      "  (15, 26)\t1\n",
      "  (15, 33)\t1\n",
      "  (15, 34)\t1\n",
      "  (15, 35)\t1\n",
      "  (16, 18)\t1\n",
      "  (16, 19)\t1\n",
      "  (16, 20)\t1\n",
      "  (16, 21)\t1\n",
      "  (16, 22)\t1\n",
      "  (16, 23)\t1\n",
      "  (16, 24)\t1\n",
      "  (16, 25)\t1\n",
      "  (16, 26)\t1\n",
      "  (16, 33)\t1\n",
      "  (16, 34)\t1\n",
      "  (16, 35)\t1\n",
      "  (17, 18)\t1\n",
      "  (17, 19)\t1\n",
      "  (17, 20)\t1\n",
      "  (17, 21)\t1\n",
      "  (17, 22)\t1\n",
      "  (17, 23)\t1\n",
      "  (17, 24)\t1\n",
      "  (17, 25)\t1\n",
      "  (17, 26)\t1\n",
      "  (17, 33)\t1\n",
      "  (17, 34)\t1\n",
      "  (17, 35)\t1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "n_camera = 3\n",
    "n_point = 3\n",
    "camera_iter=np.arange(n_camera)\n",
    "camera_idx = np.array([camera_iter]*n_point)\n",
    "camera_idx=camera_idx.T.flatten()\n",
    "point_iter=np.arange(n_point)\n",
    "point_idx = np.array([point_iter]*n_camera)\n",
    "point_idx=point_idx.flatten()\n",
    "\n",
    "print(\"camera_idx = \",camera_idx,end=\"\\n\")\n",
    "print(\"point_idx = \", point_idx,end=\"\\n\")\n",
    "\n",
    "def bundle_adjustment_sparsity(n_cameras, n_points, camera_indices, point_indices):\n",
    "    m = 2* camera_indices.size \n",
    "    n = n_cameras * 9 + n_points * 3\n",
    "    A = lil_matrix((m, n), dtype=int)\n",
    "\n",
    "    camera_indices = np.sort(camera_indices)\n",
    "    point_indices = np.sort(point_indices)\n",
    "    \n",
    "    i = np.arange(camera_indices.size)\n",
    "    \n",
    "    for s in range(9):\n",
    "        A[2 * i, camera_indices * 9 + s] = 1\n",
    "        A[2 * i + 1, camera_indices * 9 + s] = 1\n",
    "    for s in range(3):\n",
    "        A[2 * i, n_cameras * 9 + point_indices * 3 + s] = 1\n",
    "        A[2 * i + 1, n_cameras * 9 + point_indices * 3 + s] = 1\n",
    "    return A\n",
    "\n",
    "print(\"Sparse Jacobian is :\")\n",
    "J=bundle_adjustment_sparsity(n_camera, n_point, camera_idx, point_idx)\n",
    "print(J)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initializing R,t and 3D points for SfM given 2 images (`4 mark`)\n",
    "\n",
    "Using OpenCV functions, mention how you would initialize R,t (poses) and 3D points for SfM given 2 images and K matrix. You don't need to implement it, just mention function names with input/output arguments clearly and briefly explain what they do (You don't need to give detailed answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer2.* \n",
    "Given: 2 images and K matrix \n",
    "1. **First we do feature matching** \n",
    "For doing this we will create an instance of ORB detector class (in openCV ofc) with the help of `cv2.ORB_create()`. \n",
    "Then we use `detectAndCompute()` as a fuction to detector for extracting ORB features which constitues descriptors as well as keypoints on both images.(we pass none for showing we are not passing the features that are predefined) \n",
    "\n",
    "    ``` orb=cv2.ORB_create() ``` \n",
    "    ``` Keypoint1,descriptor1=orb.detectAndCompute(image1,None) ``` \n",
    "    ``` Keypoint2,descriptor2=orb.detectAndCompute(image2,None) ``` \n",
    "\n",
    "2. After this , we need to do **matching of the keypoints from the two images.** \n",
    "For doing this we will create brute-force matcher with the help of `cv2.BFMatcher()` \n",
    "Then with the help of ORB descriptors we will perform KNN matching with the help of `knnMatch()` \n",
    "And now we have the matching features from both the images. \n",
    "\n",
    "    ```brute_force=cv2.BFMatcher() ```\n",
    "    ```Matches=brute_force.knnMatch(descriptor1,descriptor2,k=2) ```\n",
    "\n",
    "    [we can also use ```brute_force.match(descriptor1,descriptor2)```]\n",
    "\n",
    "3. From the above two steps, we have matched features from both the images. Now with the help of these matches, we find fundamental matrix with the help of cv2.finfFundamentalMat()\n",
    "\n",
    "    [ Note: we can also use normalized 8 point algo ]\n",
    "\n",
    "4. After we have `F` and since we know `K` we can now have the essential matrix as $ E= K^TFK $ .\n",
    "\n",
    "\n",
    "Note: we can also combine points 3 and 4 in the following ways:\n",
    "\n",
    "We can get Essential matrix directly with the help of `E=cv2.findEssentialMatrix(points1,points2,K)` Here, we input the corresponding points in the two images (points1,points2) and also the K matrix.\n",
    "Now we have `E`, we can decompose it into R,t as \n",
    "`output=cv2.recoverPose(E,points1,points2,K) ` . Here we input `E` , matches `(points1,points2)` and calibration matrix `K` and gives `R` which is output[1] and `t` from output[2]. \n",
    "\n",
    "Then we use triangulation to initialize 3D poses and use `points_3D=cv2.triangulatePoints([points1,points2,output])` and so we get the 3D points as output !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
