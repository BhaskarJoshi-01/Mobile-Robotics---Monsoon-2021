{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundle Adjustment\n",
    "\n",
    "Part of this assignment is based on scipy-cookbook. It will take around 2 hours to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the paper 'Building Rome in a Day' and briefly write about the fundamental idea behind the problem and solution. No need to be verbose, just write about the challenge with the task and how the pipeline is implemented (do not include details about performance/parallelization).\n",
    "\n",
    "2. How is this task different from a SLAM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer1. \n",
    "The fundamental idea of the problem beign proposed is that it is uncalibrated , unstructured , large and fast image matching and 3d reconstruction of a large dataset of images and  found by searching for Rome city on Internet sharing photo sites. The images are uncalibrated and without any gps data ,just matching similar images and construct 3d map with the help of matched images. Also the results are generated within a day in fast and effiecient manner. \n",
    "Results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day.\n",
    "- Firstly they used SIFT for the extraction of image features\n",
    "- Used Nearest-neighbour search for getting the correspondences for pair of images with those extracted features.\n",
    "- Then a graph was constructed having images as vertices and there was an edge for similar images only , for this they randomly proposed an edge between vertices and only kept those which satisfy the similarity threshold.\n",
    "- The previous portion is based on entire picture similarity, in which they treated images as documents and mapped characteristics to 'visual words'.\n",
    "- Then they employed the TF-IDF-based document retrieval system to find the best/most comparable k1 + k2 photos to the current image under consideration. \n",
    "- Following that, they verified just a subset of the best k1 suggested edges.\n",
    "- Further imporvement to this was added by decreasing the no. of connected components and verifying edges from remaining k2 suggestions\n",
    "- At last the query expansion was applied where all vertices by distance of 2 edges are connected by direct edge and new edge is verified before insertion.\n",
    "- Now the track of features was generated \n",
    "- For making the above mentioned change , they create a graph with all of the features from all of the photos as vertices and connections connecting matching/similar features.(connected corresponds to tracks for specific 3d points)\n",
    "- Then they selected a subset of images to eliminate duplicate images in order to minimise the number of images for quicker processing in SFM without sacrificing accuracy.\n",
    "- To acquire the 3D points and postures, they use custom SFM with bundle adjustment and the LM optimizer.\n",
    "- At last they cluster the pictures and employ a multi-view stereo algorithm to enhance estimate of the sparse 3D points derived from SFM.\n",
    "\n",
    "Answer2.\n",
    "SLAM uses images or poses from the same sensor but in the mentioned case, we had images from different sensors whose calibrations are also not known.\n",
    "In comparison to SLAM, the data is quite  scattered in above case.\n",
    "The environment that we talk about in SLAM is small as compared to the above mentioned case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "We have a set of points in real world defined by their coordinates $(X, Y, Z)$ in some apriori chosen \"world coordinate frame\". We photograph these points by different cameras, which are characterized by their orientation and translation relative to the world coordinate frame and also by focal length and two radial distortion parameters (9 parameters in total). Then we precicely measure 2-D coordinates $(x, y)$ of the points projected by the cameras on images. Our task is to refine 3-D coordinates of original points as well as camera parameters, by minimizing the sum of squares of reprojecting errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a dataset from http://grail.cs.washington.edu/projects/bal/ for this task. Feel free to choose any of the ones mentioned on the page. Take the smallest file from each dataset (you can choose any but it will take longer to run, consume more memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import copy\n",
    "import bz2\n",
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://grail.cs.washington.edu/projects/bal/data/\"\n",
    "\n",
    "DATASET_NAME = \"final/\"\n",
    "FILE_NAME = \"problem-13682-4456117-pre.txt.bz2\"\n",
    "\n",
    "URL = BASE_URL + DATASET_NAME + FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(FILE_NAME):\n",
    "    urllib.request.urlretrieve(URL, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the data from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bal_data(file_name):\n",
    "    with bz2.open(file_name, \"rt\") as file:\n",
    "        n_cameras, n_points, n_observations = map(\n",
    "            int, file.readline().split())\n",
    "\n",
    "        camera_indices = np.empty(n_observations, dtype=int)\n",
    "        point_indices = np.empty(n_observations, dtype=int)\n",
    "        points_2d = np.empty((n_observations, 2))\n",
    "\n",
    "        for i in range(n_observations):\n",
    "            camera_index, point_index, x, y = file.readline().split()\n",
    "            camera_indices[i] = int(camera_index)\n",
    "            point_indices[i] = int(point_index)\n",
    "            points_2d[i] = [float(x), float(y)]\n",
    "\n",
    "        camera_params = np.empty(n_cameras * 9)\n",
    "        for i in range(n_cameras * 9):\n",
    "            camera_params[i] = float(file.readline())\n",
    "        camera_params = camera_params.reshape((n_cameras, -1))\n",
    "\n",
    "        points_3d = np.empty(n_points * 3)\n",
    "        for i in range(n_points * 3):\n",
    "            points_3d[i] = float(file.readline())\n",
    "        points_3d = points_3d.reshape((n_points, -1))\n",
    "\n",
    "    return camera_params, points_3d, camera_indices, point_indices, points_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_params, points_3d, camera_indices, point_indices, points_2d = read_bal_data(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"camera_params: {camera_params.shape};\\npoints_3d: {points_3d.shape};\\n\"\n",
    "        f\"camera_indices: {camera_indices.shape}; \\npoint_indices: {point_indices.shape}; \\n\"\n",
    "        f\"points_2d: {points_2d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have numpy arrays:\n",
    "\n",
    "1. `camera_params` with shape `(n_cameras, 9)` contains initial estimates of parameters for all cameras. First 3 components in each row form a **rotation vector**, next 3 components form a translation vector, then a focal distance and two distortion parameters.\n",
    "2. `points_3d` with shape `(n_points, 3)` contains initial estimates of point coordinates in the world frame.\n",
    "3. `points_2d` with shape `(n_observations, 2)` contains measured 2-D coordinates of points projected on images in all the observations.\n",
    "4. `camera_ind` with shape `(n_observations,)` gives the index of the camera (from 0 to `n_cameras - 1`) associated with a particular observation.   \n",
    "5. `point_ind` with shape `(n_observations,)` contains indices of 3D points (from 0 to `n_points - 1`) involved in each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Point Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise `points_3d`. It may not look like 'Venice' or any building as we are working with a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many cameras and 3D points do we have? Calculate the number of parameters to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_cameras = None\n",
    "n_points = None\n",
    "m = None\n",
    "n = None\n",
    "\n",
    "print(\"n_cameras: {}\".format(n_cameras))\n",
    "print(\"n_points: {}\".format(n_points))\n",
    "print(\"Total number of parameters to estimate: {}\".format(n))\n",
    "print(\"Total number of residuals: {}\".format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose a relatively small problem to reduce computation time, but scipy's algorithm is capable of solving much larger problems, although required time will grow proportionally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the function which returns a vector of residuals. We use numpy vectorized computations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short review on Transformations\n",
    "\n",
    "Rodrigues Formula: $$\\mathbf{R}=\\cos \\theta \\mathbf{I}+(1-\\cos \\theta) \\mathbf{n n}^{\\mathrm{T}}+\\sin \\theta \\mathbf{n}^{\\wedge}$$\n",
    "If described by a rotation vector, assuming that the rotation axis is a unit length vector $\\mathbf{n}$ and the angle is $\\theta$, then the vector $\\theta \\mathbf{n}$ can also describe this rotation. Here, rot_vecs = $\\theta \\mathbf{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(points, rot_vecs):\n",
    "    \"\"\"Rotate points by given rotation vectors.\n",
    "    \n",
    "    Rodrigues' rotation formula is used.\n",
    "    \"\"\"\n",
    "    theta = np.linalg.norm(rot_vecs, axis=1)[:, np.newaxis] #np.newaxis converts this into a column vector.\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        v = rot_vecs / theta\n",
    "        v = np.nan_to_num(v)\n",
    "    dot = np.sum(points * v, axis=1)[:, np.newaxis]\n",
    "    cos_theta = np.cos(theta)\n",
    "    sin_theta = np.sin(theta)\n",
    "    \n",
    "    return (cos_theta * points) + ((1 - cos_theta) * v * dot) + (sin_theta * np.cross(v, points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short review on camera modelling & radial distortion\n",
    "\n",
    "\n",
    "\n",
    "- Each pixel moves radially away from (barrel) or towards (pincushion) the image center (c).\n",
    "- As a function of distance from $c: r_{c}^{2}=x_{c}^{2}+y_{c}^{2}$.\n",
    "- The shift $\\gamma$ can be modelled as: $\\gamma=1+k_{1} r_{c}^{2}+k_{2} r_{c}^{4}$ where ${k}_{1}$ and ${k}_{2}$ are radial distortion parameters.\n",
    "- The modified co-ordinates are:\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\hat{x}_{c}=\\gamma x_{c} \\\\\n",
    "\\hat{y}_{c}=\\gamma y_{c}\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "- **This is applied before the focal-length multiplier and center shift are applied**: Meaning before $K$ matrix is even applied. But how do we exactly do that?\n",
    "\n",
    "    $$\\mathbf{K}=\\left[\\begin{array}{ccc}\\alpha_{x} & 0 & x_{0} \\\\0 & \\alpha_{y} & y_{0} \\\\0 & 0 & 1\\end{array}\\right] ; \\qquad      \\lambda {p} = \\mathrm{x} =K[R \\quad t] \\mathrm{X}$$\n",
    "\n",
    "    $$x_{final} = \\gamma \\left(\\frac{f_0X}{Z}+c_x \\right)\n",
    "     \\qquad \\color{red} \\bigotimes \\textbf{wrong}$$\n",
    "\n",
    "    $$x_{final} =  \\left(f_0 \\left(\\gamma\\frac{X}{Z} \\right)+c_x \\right)\n",
    "     \\qquad \\color{surd} \\checkmark \\textbf{correct}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing it up\n",
    "Let $\\pmb{P} = (X, Y, Z)^T$ - a radius-vector of a point, $\\pmb{R}$ - a rotation matrix of a camera, $\\pmb{t}$ - a translation vector of a camera, $f$ - its focal distance, $k_1, k_2$ - its distortion parameters. Then the reprojecting is done as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\pmb{Q} = \\pmb{R} \\pmb{P} + \\pmb{t} \\\\\n",
    "\\pmb{q} = -\\begin{pmatrix} Q_x / Q_z \\\\ Q_y / Q_z \\end{pmatrix} \\\\\n",
    "\\pmb{p} = f (1 + k_1 \\lVert \\pmb{q} \\rVert^2 + k_2 \\lVert \\pmb{q} \\rVert^4) \\pmb{q}\n",
    "\\end{align}\n",
    "The resulting vector $\\pmb{p}=(x, y)^T$ contains image coordinates of the original point.\n",
    "![radial_distortion_1.png](./misc/radial_distortion_1.png) \n",
    "![radial_distortion_2.png](./misc/radial_distortion_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(points, camera_params):\n",
    "    \"\"\"Convert 3-D points to 2-D by projecting onto images.\"\"\"\n",
    "    \n",
    "    \n",
    "    #############################\n",
    "    #\n",
    "    # TO DO : Implement this function based on the information mentioned above.\n",
    "    #\n",
    "    #############################\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(params, n_cameras, n_points, camera_indices, point_indices, points_2d):\n",
    "    \"\"\"Compute residuals.\n",
    "    \n",
    "    `params` contains camera parameters and 3-D coordinates.\n",
    "    \"\"\"\n",
    "    params = copy.deepcopy(params)\n",
    "    camera_params = params[:n_cameras * 9].reshape((n_cameras, 9))\n",
    "    \n",
    "    points_3d = params[n_cameras * 9:].reshape((n_points, 3))\n",
    "    points_proj = project(points_3d[point_indices], camera_params[camera_indices])\n",
    "    return (points_proj - points_2d).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short review on Structure from Motion\n",
    "### Residual\n",
    "In our lecture, in the residual vector, we  wrote the elements in order: 11, 12, 13.., 1N, then 21, 22.. and so on till MN. However, notice that it is not the case here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M -> camera, N -> 3D point (in our lectures, NOT in this code)\n",
    "![sfm_residual_1.png](./misc/sfm_residual_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that computing Jacobian of `fun` is cumbersome, thus we will rely on the finite difference approximation. To make this process time feasible we provide Jacobian sparsity structure (i. e. mark elements which are known to be non-zero):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sfm_jac_2.png](./misc/sfm_jac_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the matrix is sparse, we can make use of datastructures that are meant for such a usecase - https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the matrix computation has been given to you, you will have to explain this function later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bundle_adjustment_sparsity(n_cameras, n_points, camera_indices, point_indices):\n",
    "    m = None\n",
    "    n = None\n",
    "            \n",
    "    A = lil_matrix((m, n), dtype=int)\n",
    "\n",
    "    camera_indices = np.sort(camera_indices)\n",
    "    point_indices = np.sort(point_indices)\n",
    "    \n",
    "    i = np.arange(camera_indices.size)\n",
    "    for s in range(9):\n",
    "        A[2 * i, camera_indices * 9 + s] = 1\n",
    "        A[2 * i + 1, camera_indices * 9 + s] = 1\n",
    "\n",
    "    for s in range(3):\n",
    "        A[2 * i, n_cameras * 9 + point_indices * 3 + s] = 1\n",
    "        A[2 * i + 1, n_cameras * 9 + point_indices * 3 + s] = 1\n",
    "            \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THAT'S IT! Now we are ready to use inbuilt library functions!\n",
    "Now we are ready to run optimization. Let's visualize residuals evaluated with the initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.hstack((camera_params.ravel(), points_3d.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = fun(x0, n_cameras, n_points, camera_indices, point_indices, points_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = bundle_adjustment_sparsity(n_cameras, n_points, camera_indices, point_indices)\n",
    "print(A.shape, n_cameras, n_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Scipy has existing functions for optimization that we can make use of. Write a sentence about the method that is used for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# So far: method='lm'\n",
    "res = least_squares(fun, x0, jac_sparsity=A, verbose=2, x_scale='jac', ftol=1e-4, method='trf',\n",
    "                    args=(n_cameras, n_points, camera_indices, point_indices, points_2d))\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = res.x\n",
    "\n",
    "new_camera_params = params[:n_cameras * 9].reshape((n_cameras, 9))\n",
    "new_points_3d = params[n_cameras * 9:].reshape((n_points, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Optimised Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(new_points_3d)\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `scaling='jac'` was done to automatically scale the variables and equalize their influence on the cost function (clearly the camera parameters and coordinates of the points are very different entities). This option turned out to be crucial for successfull bundle adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimization took {0:.0f} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot residuals at the found solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see much better picture of residuals now, with the mean being very close to zero. There are some spikes left. It can be explained by outliers in the data, or, possibly, the algorithm found a local minimum (very good one though) or didn't converged enough. Note that the algorithm worked with Jacobian finite difference aproximate, which can potentially block the progress near the minimum because of insufficient accuracy (but again, computing exact Jacobian for this problem is quite difficult)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2- Part B: Submission details -\n",
    "You are supposed to gain understanding by playing around with the code above and submit your answers to questions asked below. You shouldn't submit this whole notebook, just copy the following cells (starting next cell up until the end of this notebook) and paste it at the end of your Project 2 notebook (already shared on GitHub classrooms, [link](https://github.com/AryanSakaria/Project_2/blob/main/Project_2.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "## 1. SfM pipeline (`6 mark`)\n",
    "\n",
    "To get the context of below questions, take a look at the code above: The same questions have been asked at different places above as comments in the code.\n",
    "\n",
    "1. `0.5 mark` **Basics** - How do we know this (`camera_ind`) information in practical setting? In other words, how do we know observations in `points_2d` belong to which camera. Explain. \n",
    "    - Ans-1 - Basics: In practical setting  we can correlate each image with a camera pose. As a result, there is a correspondence between the picture and the camera pose. We may extend this correspondence to the image coordinates because the points recorded in points 2d are from the image. Typically, these images are obtained through a feature extractor such as SIFT.\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "2. `0.5 mark` **Basics** - How do we know this (`point_ind`) information in practical setting?  In other words, how do we know observations in `points_2d` belong to which 3D point. Explain.\n",
    "    - Ans-2 - Basics: In practical settings, we know that images in different perspectives are captured at different positions. We can use SIFT/SURF or ORB to determine corresponding points between two photos using this information. After this  8-point algorithm estimates the Fundamental Matrix, from which we may obtain the Essential Matrix and decompose it into R,t, and K. To obtain 3D coordinates, we can now triangulate or backproject points to the world. As a result, we now have our 2D-3D correspondences.\n",
    "\n",
    "\n",
    "    \n",
    "3. `0.5 mark` **Transformations** - `rotate()` function: Why do we use the rodriquez formula? How is this representation different from the standard 3x3 Rotation matrix, why do we use this instead?\n",
    "    - Ans-3 - Transformations: Rodrigue's formula defines rotation of a point $P$ as $P$ being rotated by an angle $theta$ about a fixed axis $n$. \n",
    "    This is distinct from our usual Rotation matrix, in which we convert a point by multiplying it by a 3x3 rotation matrix (R). \n",
    "    So, in the instance of Rodrigue, we don't require a 3x3 matrix\n",
    "    We only need minimum three parameters : two for the rotation vector  and one for theta. (the third can be computed from the other two because this vector is a unit vector).\n",
    "    We do this because we have a camera params matrix (3x3) with 9 parameters in this example (including the 3 parameters required for rotation by rodrigues). \n",
    "    We were given a 3x4 projection matrix with a total of 12 parameters in our lesson (this is because it contains R,t and K). \n",
    "    So in all, we have $9*n cameras$ parameters to estimate in this scenario, but in the case of a 3x3 rotation matrix, we have $12*n cameras$ parameters to estimate.\n",
    "    \n",
    "    $$\\mathbf{R}=\\cos \\theta \\mathbf{I}+(1-\\cos \\theta) \\mathbf{n n}^{\\mathrm{T}}+\\sin \\theta \\mathbf{n}^{\\wedge}$$\n",
    "\n",
    "    \n",
    "4. `0.5 mark` **Transformations** - `project()` function: In the `project()` function, would it make any difference if I do translate first, then rotate? Why/why not?\n",
    "    - Ans-4 - Transformations: Yes, there could be a distinction. Translation precedes rotation, implying that we move to a different origin first and then rotate about that origin. When we translate after rotating, we are also moving the rotation centre.\n",
    "    If the translation vector is in the camera frame, the point must first be rotated to the camera frame before being added to the translation vector; otherwise, the translation vector and the point would be in different frames.\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "5. `0.5 mark` **Jacobian** - `bundle_adjustment_sparsity()` function: m above is not \"M*N\" (*2) unlike our lecture notes. Why is that so?\n",
    "    - Ans-5 - Jacobian : As every row in Jacobian corresponds to a residue, we have as many rows as no. of residuals and that is equal to the total no. of 2d points we have as the loss function measures the diff between reporjected 2d points calc from estimated 3d points and estimated parameters and obs 2d points from image.\n",
    "    Now because each 2d point contributes 2 coordinates and not one i.e x and y , so we have twice as many residuals as the number of 2d points.\n",
    "    And it is different from class because in class we have assumed each N point is visible in M images so it leaves us with MN 2d points.Here however we have camera_indices.size 2d points as each element in camera_indices maps 2d point to image so twice as many rows or residuals in jacobian.\n",
    "    \n",
    "6. `2 mark` **Jacobian & Parameters** - `bundle_adjustment_sparsity()` function: \n",
    "    1.  Why are we doing `n_cameras * 9` here instead of `n_cameras * 12`? Recollect: Every individual motion Jacobian was (1*)12 in our lecture notes. \n",
    "        - Ans 6.1: Jacobian & Parameters: We used the homogeneous transformation matrix of dimensions 3x4 in the lecture, which gave us 12 unknowns for each camera. This matrix combines the intrinsics and the extrinsics (rotation and translation) into a single matrix, and we just optimise for it. \n",
    "        In this case, we segregated the various parameters by utilising distinct equations to describe the individual parts. \n",
    "        We employed one equation for the extrinsics, which included three rotation parameters and three translation parameters, and one equation for the intrinsics, which included three parameters (1 focal length + two radial distortion). \n",
    "        We optimised for each of these factors as a separate entity, resulting in just 9 unknowns per camera.   \n",
    "        \n",
    "    2. Ignoring the scale parameters, what was the number of unknown parameters in our lecture notes in terms of `n_cameras` and `n_points`? What is it here in the code? Is it different? If so, what is and why? [Link of notes](https://www.notion.so/Stereo-Structure-from-Motion-9fdd81e4194f4803ac9ba7552df56470).\n",
    "        - Ans 6.2: Jacobian and Parameters: We had `n points * 3 + n cameras * 12` unknowns in the lecture and   we have `n cameras * 9 + n points * 3` unknowns in the code.\n",
    "        we have `n cameras * 9 + n points * 3` unknowns in the code. The primary difference here is the quantity of unknowns associated with the poses/camera frames. \n",
    "        In the lecture, we avoided optimising over the SO(3) manifold by optimising over the projection matrix of dimensions 3x4 and only required to explicitly estimate the 12 projection parameters, which comprised both extrinsics and intrinsics in a single matrix. In contrast, we model the extrinsics and intrinsics independently in this case, employing a rotation vector (3), a translation vector (3), and three extra parameters of focal length (1) and radial distortion (2), totaling nine unknowns for each view of the camera.         \n",
    "            \n",
    "            \n",
    "7. `6 mark` **Sparsity, Residual Vector & Jacobian** - `bundle_adjustment_sparsity()` function: Explain what you understand from above 6 lines of code by coding a simple toy example yourself to illustrate how it is different from what you've learnt in class. ([Coding toy example + elaborating in words]- both are compulsory.) For the toy example, you can take something like 3 points all seen from 3 cameras. (You don't actually have to code much, just need to call the existing function) Write that toy example after this cell\n",
    "    - Ans 6 - Sparsity, Residual Vector & Jacobian: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initializing R,t and 3D points for SfM given 2 images (`4 mark`)\n",
    "\n",
    "Using OpenCV functions, mention how you would initialize R,t (poses) and 3D points for SfM given 2 images and K matrix. You don't need to implement it, just mention function names with input/output arguments clearly and briefly explain what they do (You don't need to give detailed answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer2.* \n",
    "Given: 2 images and K matrix \n",
    "1. **First we do feature matching** \n",
    "For doing this we will create an instance of ORB detector class (in openCV ofc) with the help of `cv2.ORB_create()`. \n",
    "Then we use `detectAndCompute()` as a fuction to detector for extracting ORB features which constitues descriptors as well as keypoints on both images.(we pass none for showing we are not passing the features that are predefined) \n",
    "\n",
    "    ``` orb=cv2.ORB_create() ``` \n",
    "    ``` Keypoint1,descriptor1=orb.detectAndCompute(image1,None) ``` \n",
    "    ``` Keypoint2,descriptor2=orb.detectAndCompute(image2,None) ``` \n",
    "\n",
    "2. After this , we need to do **matching of the keypoints from the two images.** \n",
    "For doing this we will create brute-force matcher with the help of `cv2.BFMatcher()` \n",
    "Then with the help of ORB descriptors we will perform KNN matching with the help of `knnMatch()` \n",
    "And now we have the matching features from both the images. \n",
    "\n",
    "    ```brute_force=cv2.BFMatcher() ```\n",
    "    ```Matches=brute_force.knnMatch(descriptor1,descriptor2,k=2) ```\n",
    "\n",
    "    [we can also use ```brute_force.match(descriptor1,descriptor2)```]\n",
    "\n",
    "3. From the above two steps, we have matched features from both the images. Now with the help of these matches, we find fundamental matrix with the help of cv2.finfFundamentalMat()\n",
    "\n",
    "    [ Note: we can also use normalized 8 point algo ]\n",
    "\n",
    "4. After we have `F` and since we know `K` we can now have the essential matrix as $ E= K^TFK $ .\n",
    "\n",
    "\n",
    "Note: we can also combine points 3 and 4 in the following ways:\n",
    "\n",
    "We can get Essential matrix directly with the help of `E=cv2.findEssentialMatrix(points1,points2,K)` Here, we input the corresponding points in the two images (points1,points2) and also the K matrix.\n",
    "Now we have `E`, we can decompose it into R,t as \n",
    "`output=cv2.recoverPose(E,points1,points2,K) ` . Here we input `E` , matches `(points1,points2)` and calibration matrix `K` and gives `R` which is output[1] and `t` from output[2]. \n",
    "\n",
    "Then we use triangulation to initialize 3D poses and use `points_3D=cv2.triangulatePoints([points1,points2,output])` and so we get the 3D points as output !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
